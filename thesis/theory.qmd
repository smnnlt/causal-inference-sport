---
title: ''
format: pdf
bibliography: ../references.bib
execute: 
  echo: false
---

```{r}
#| label: setup
#| warning: false
library(dagitty)
library(ggdag)
library(ggplot2)
library(patchwork)

# load simulation functions
source("../scripts/simulation.R")
```

# Theoretical Background

## Causility, Associations, and (In)dependence

## Graphical Causal Models

Graphical models are an easy way to conceptualize causal systems. Pioneered by XXXX and YYYY, they allow to visualize causal relationships, which eases development and understanding of causal models.

A graphical causal model visualizes the exposure, outcome, covariates, and their (assumed) causal relationship. In the following, we will usually note the exposure (or independent variable) with $X$, the outcome (or dependent variable) with $Y$, and covariates (all other variables) with other letters. Note, that depending on the research question, the assignment of exposure, outcome, and covariates may change within the same model, so there can be instances, where the conventional naming cannot be followed.

Variables in graphical causal model are connected by arrows. An arrow between $X$ and $Y$ means, that a direct causal relationship between the two is assumed (see @fig-dag1) . The direction of the arrow tells the direction of causality. As in @fig-dag1, $X \rightarrow Y$ means that $X$ causes $Y$ (and not the other way around). For our definition of causality this means, if we intervene on $X$ we expect to see a change in $Y$.

The direction of causality has to be determined by theoretical knowledge. It cannot be found in the data itself. Suppose that in our first example in @fig-dag1, $X$ is biological sex and $Y$ is endurance performance. It appears obvious, that a causal relationship between both exist (though it is certainly much more complicated than that seen in the simple model). However, the fact that it is sex that causes performance --- and not the other way around --- is based purely on theoretical knowledge and understanding of the world. There are neither randomized trials for proof (because you cannot randomly assign sex), nor controlled interventions (because you cannot easily intervene on sex) possible. Ultimately, the direction of causality is an assumption by the researcher.

```{r}
#| label: fig-dag1
#| out-width: '50%'
#| fig-scap: "A simple graphical causal model with two variables."
#| fig-cap: "A simple graphical causal model with two variables. The variable $X$ (exposure) is assumed to cause the variable $Y$ (outcome). No other variables are believed to influence this process."

dag1 <- dagify(Y ~ X)
coordinates(dag1) <- list(x = c(X = 0, Y = 1), y = c(X = 0, Y = 0))
ggdag_classic(dag1, size = 15) + theme_dag_blank()

```

Causal systems in the world are usually more complex than consisting of only exposure and outcome, and so are the graphical causal models depicting them. A more complex graph is displayed in @fig-dag2. $X$ and $Y$ are not directly connected anymore, but indirectly via $B$. This is called a *causal path*. We will later see, that some models also have non-causal paths.

```{r}
#| label: fig-dag2
#| out-width: '50%'
#| fig-scap: "A more complex graphical causal model with four variables."
#| fig-cap: "A more complex graphical causal model with four variables. $X$ and $A$ both cause $B$, which in turn causes $Y$."

dag2 <- dagify(Y ~ B, B ~ X + A)
coordinates(dag2) <- list(x = c(X = 0, A = 0, B = 0.5, Y = 1), y = c(X = 1, A = 0, B = 0.5, Y = 0.5))
ggdag_classic(dag2, size = 15) + theme_dag_blank()

```

The graph in @fig-dag2 is called an directed acyclic graph (DAG). It is directed, because all paths have arrows (the direction of causality is set). It is acyclic, because there are no circular paths in it. Finally, it is a graph. All graphs in this thesis will be DAGs, because the presented tools work only for these, and most research problems can be adequately formulated using them.

What is inside a DAG is as important as what is not inside it. A DAG should depict all causal relations important to the research question. If two variables are not connected, we assume that they do not causally relate to each other. For example, in @fig-dag2, there is no direct connection between $X$ and $A$, or between $X$ and $Y$.

DAGs tell a story. For example, we can assign the variables in @fig-dag2 to a very simple model of endurance performance. Let $X$ be the biological sex, $A$ the nutrition status, $B$ the physiological capacity to perform endurance tasks, and $Y$ the endurance performance in a competition. Our model assumes that sex and nutrition both directly affect the physiological capacity, and this in turn affects the performance. On the other hand, it assumes that sex and nutrition are not causally related, and that both sex and nutrition have no direct effect on performance, but only an indirect effect via physiological capacity.

## Error Terms in Causal Modeling

If we knew the true causal model and could measure all variables perfectly, we could exactly determine all causal effects. In reality, this is impossible. One of the main reasons are unobserved factors, that influence our relevant variables in the model. This could be things like random measurement error or biological variability. Taken together with the fact, that we can always only investigate causal effects in a sample of the population, our research will only result in an estimate of the causal effect. 

As with any statistical analysis, we aim for unbiased and precise estimates. Unbiased means, that on average our estimate will correspond to the true value of the variable. Precise means, that the estimate will have a small variance, or in other words, that a single estimate is sufficiently near to the average estimate. Random error terms add imprecision, but not bias to our model. We will later learn scenarios, that introduce bias.

To demonstrate the concepts of precision and bias of causal effect estimated, I will use toy data simulations in the following. These simulations create samples (n = 100) of data corresponding to the simulated causal model including random error terms. Each simulated sample is modeled to yield a single causal effect estimate. I then visualize the distribution of simulated estimated. More details of the simulation procedure can be found in @sec-sim. 

@fig-dag3 demonstrates how unobserved factors (random error terms) add uncertainty to a causal effect estimate. Without the random error term, each sample would give the exact true causal effect. With the random error terms, some samples will give estimates that differ from the true causal effect. One main goal of causal inference is to create models, whose estimates do not differ systematically and do not differ too much from the true effect.

```{r}
#| label: fig-dag3
#| out-width: '50%'
#| fig-height: 6
#| fig-scap: "A simple causal path, with random error."
#| fig-cap: "A simple causal path, with random error. (a) $X$ causes $Y$, but both variables are influenced by other unobserved variables (random error). (b) A simulation of the model. The density plot shows the distribution of k = 1000 simulations of the model with random error terms. The random error adds uncertainty to the estimate of the causal effect, but no bias (i.e., on average, the true causal effect can be correctly estimated)."

dag3 <- dagify(Y ~ X + U2, X ~ U1)
coordinates(dag3) <- list(x = c(X = 0, Y = 1, U1 = 0, U2 = 1), y = c(X = 0, Y = 0, U1 = 1, U2 = 1))
p3.1 <- ggdag_classic(dag3, size = 15) + theme_dag_blank()

v <- sim(base)
p3.2 <- ggplot() +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_vline(xintercept = 1, linewidth = 1.2) +
  geom_density(aes(x = v), color = "royalblue", linewidth = 1.2) +
  labs(x = "effect of X on Y", y = NULL) +
  xlim(c(0,2)) +
  annotate("text", label = "true effect", x = 1.2, y = max(density(v)$y)) +
  annotate("text", label = "estimated effect", x = 1.35, y = max(density(v)$y) * 0.8, color = "royalblue") +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.y = element_blank()
  )

p3.1 / p3.2 + plot_annotation(tag_levels = "a")

```

Certainty in causal effect estimates is higher in simpler models. The main reason for this is that simpler models have less random error terms. This can be demonstrated by comparing a simple causal relation with a causal path (a chain).

Along a causal path, information is generally lost, even if the causal effects are unaltered. The culprits are the additional error terms of intermediate variables (see @fig-dag4). Chains therefore introduce uncertainty, but no bias, to a causal effect estimate.

```{r}
#| label: fig-dag4
#| out-width: '50%'
#| fig-height: 6
#| fig-width: 8
#| fig-scap: "Random errors in a causal path."
#| fig-cap: "Random errors in a causal path. (a) $X$ causes $Y$ directly. Both variables are influenced by random errors. (b) $X$ causes $Y$ via $A$. All three variables are influenced by random errors. (b) A simulation of the effect of $X$ on $Y$ in both models. The chain introduces additional uncertainty in the effect estimate, but no bias."

dag4 <- dagify(X ~ U1, A ~ X + U2, Y ~ A + U3)
coordinates(dag4) <- list(x = c(X = 0, A = 0.5, Y = 1, U1 = 0, U2 = 0.5, U3 = 1), y = c(X = 0, A = 0, Y = 0, U1 = 1, U2 = 1, U3 = 1))
p4.1 <- ggdag_classic(dag4, size = 15) + theme_dag_blank()

v1 <- sim(base)
v2 <- sim(chain)

p4.3 <- ggplot() +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_vline(xintercept = 1, linewidth = 1.2) +
  geom_density(aes(x = v1), color = "royalblue", linewidth = 1.2) +
  geom_density(aes(x = v2), color = "darkgreen", linewidth = 1.2) +
  labs(x = "effect of X on Y", y = NULL) +
  xlim(c(0,2)) +
  annotate("text", label = "true effect", x = 1.2, y = max(density(v)$y)) +
  annotate("text", label = "estimated effect (a)", x = 1.35, y = max(density(v)$y) * 0.8, color = "royalblue") +
  annotate("text", label = "estimated effect (b)", x = 1.5, y = max(density(v)$y) * 0.5, color = "darkgreen") +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.y = element_blank()
  )

(p3.1 + p4.1) / p4.3 + plot_annotation(tag_levels = "a")

```

For an example from sport science think of two different causal effects. First, the effect of a running intervention on mitochondrial density. Second, the effect of a running intervention on endurance performance. Even if we assume in the second case, that the effect is directly chained trough mitochondrial density (i.e., $intervention \rightarrow density \rightarrow performance$), the effect on endurance performance is harder to estimate. The main reason is, that endurance performance will be influenced by additional unobserved factors, that will not influence mitochondrial density, for example motivation, pacing, or day-to-day variability.

Viewing the causal model in @fig-dag4, we have to reconsider that the arrows drawn in a DAG are as interesting as the arrows not drawn. In the example, all unobserved error terms are parent nodes, meaning that they are not influenced by any relevant variable, so also not by each other. This is a general assumption regarding unobserved error terms: We assume random errors to be uncorrelated. As soon as errors influence each other (directly or via other variables), we should model them explicitly to yield the best estimates. 
<!-- maybe refer to previous example -->

## Modeling Causal Systems

DAGs can be understood as linear models.

DAGs are an abstract concept to describe research problems. This level of abstraction allows to plan a study and its data analysis on a conceptual level. For the actual data analysis, a DAG has to be filled with data and functions. 

Arrows in a DAG represent a causal relationship, but they do not specify the type of this relationship. $X \rightarrow Y$ can mean a strong linear affect of $X$ on $Y$, or a weak negative curvelinear relation. While the researcher is free to choose what exact type of relation a DAG assumes for a set of two variables, a linear relationship is the common choice. 

```{r}
#| label: fig-dag8
#| out-width: '50%'
#| fig-height: 6
#| fig-scap: "A causal path blocked by conditioning."
#| fig-cap: "A causal path blocked by conditioning. (a) $X$ causes $Y$ via $A$. (b) The causal path is blocked, because the analysis conditions on $A$. As all affects of $X$ on $Y$ trail through $A$, no causal effect remains. (c) A simulation of the effect of $X$ on $Y$ in both models. Blocking removes the true causal effect entirely."

dag8 <- dagify(A ~ X, Y ~ A)
coordinates(dag8) <- list(x = c(X = 0, A = 0.5, Y = 1), y = c(X = 0, A = 0, Y = 0))
p8.1 <- ggdag_classic(dag8, size = 15) + theme_dag_blank()
p8.2 <- ggdag_classic(dag8, size = 15) + theme_dag_blank() + annotate("text", label = "A", x = 0.5, y = 0, color = "grey80", size = 15)

v2 <- sim(chain)
v3 <- sim(chain, block = TRUE)

p8.3 <- ggplot() +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_vline(xintercept = 1, linewidth = 1.2) +
  geom_density(aes(x = v3), color = "grey80", linewidth = 1.2) +
  geom_density(aes(x = v2), color = "royalblue", linewidth = 1.2) +
  labs(x = "effect of X on Y", y = NULL) +
  xlim(c(-1,3)) +
  annotate("text", label = "true effect", x = 1.4, y = max(density(v2)$y)) +
  annotate("text", label = "blocked effect (b)", x = -0.6, y = max(density(v2)$y) * 0.9, color = "grey80") +
  annotate("text", label = "estimated effect (a)", x = 1.8, y = max(density(v2)$y) * 0.5, color = "royalblue") +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.y = element_blank()
  )

(p8.1 + p8.2) / p8.3 + plot_annotation(tag_levels = "a")


```

## Confounders and Colliders

Confounders are variables that influence both the exposure and the outcome causally (see @fig-dag9 a). The confounder creates a spurious (non-causal) association between exposure and outcome. Conceptually, a confounder gives a set of similar information (knowledge) to both exposure and outcome. This leads to both sharing a set of information, regardless of their actual causal relationship. The actual causal relationship is biased.

Confounders can be controlled for by conditioning on them in a model. This removes the entire bias and preserves the actual causal relationship. 

Let's take an example by looking at @fig-dag9. We are interested in the relationship between the (average) 5000-m time trial speed and the (average) 100-m sprint speed. We assume, that being fast in an endurance task reduces the ability to sprint fast, and thus reduces the 100-m speed. Therefore, we are interested in the causal relationship between $X$ (endurance speed) and $Y$ sprinting speed. Note that this is a very simplistic causal model, as we could also model the unobserved ability to sprint and ability to perform endurance tasks, as well as their potential causes. 

Our model has a collider $A$, the biological sex. From expert knowledge, we know that sex causally influences both sprinting and endurance performance, mainly via anthropometry and physiology. Sex thus biases the causal relationship between sprinting and endurance performance. To remove this bias, the analysis has to control for sex. For a discrete variable such as sex is typically documented as, controlling for means in practice stratifying the analysis by it. Assuming our causal model is correct --- which holds of course not true in our simple example here --- controlling for sex gives us the true (unbiased) causal relationship between endurance and sprinting performance.

```{r}
#| label: fig-dag9
#| out-width: '50%'
#| fig-height: 6
#| fig-scap: "A graphical example of confounding."
#| fig-cap: "A graphical example of confounding. Both $X$ and $Y$ share a common cause $A$. (a) This confounder biases determining the causal effect of $X$ on $Y$. (b) Conditioning on $A$ removes the bias in the analysis. (c)  A simulation of the effect of $X$ on $Y$ in both models."

dag9 <- dagify(X ~ A, Y ~ A + X)
coordinates(dag9) <- list(x = c(X = 0, A = 0.5, Y = 1), y = c(X = 0, A = 1, Y = 0))
p9.1 <- ggdag_classic(dag9, size = 15) + theme_dag_blank(plot.background = element_rect(color = "#B2423B", linewidth = 3))
p9.2 <- ggdag_classic(dag9, size = 15) + theme_dag_blank() + annotate("text", label = "A", x = 0.5, y = 1, color = "grey80", size = 15)

v4 <- sim(conf)
v5 <- sim(conf, block = TRUE)

p9.3 <- ggplot() +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_vline(xintercept = 1, linewidth = 1.2) +
  geom_density(aes(x = v5), color = "royalblue", linewidth = 1.2) +
  geom_density(aes(x = v4), color = "darkred", linewidth = 1.2) +
  labs(x = "effect of X on Y", y = NULL) +
  xlim(c(0,2)) +
  annotate("text", label = "true effect", x = 1.15, y = max(density(v4)$y)) +
  annotate("text", label = "estimated effect (b)", x = 0.65, y = max(density(v4)$y) * 0.7, color = "royalblue") +
  annotate("text", label = "biased effect (a)", x = 1.85, y = max(density(v4)$y) * 0.6, color = "darkred") +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.y = element_blank()
  )

(p9.1 + p9.2) / p9.3 + plot_annotation(tag_levels = "a")




```

Colliders pose a more subtle form of bias. A collider is a variable that is causally influenced by the exposure and the outcome (see @fig-dag10 a). Per se, colliders do not cause harm. But when they are conditioning on the introduce bias into a model. This collider bias can be understood by the following: A collider combines knowledge from both its source, the exposure and the outcome, and thus also of their causal relationship. If this combined knowledge is being removed from a model by conditioning on the collider, then some of the actual causal relationship between exposure and outcome is also removed.

Consider the causal relationship between $X$ post-lactate in a ramp test and $Y$ maximum oxygen uptake in the same ramp test. Basically, our question is if more lactate causes a higher or lower maximum oxygen uptake. In our model, both lactate and VO2max influence the maximum velocity in the ramp test. This appears reasonable, as individuals with a more capable glycolytic or oxidatative energy metabolism are likely to outperform their counterparts that have neither in term of the maximum velocity. The maximum velocity is thus the collider $B$. Conditioning on it will bias our model.<!-- explain why! -->

```{r}
#| label: fig-dag10
#| fig-height: 6
#| out-width: '50%'
#| fig-scap: "A graphical example of collider bias."
#| fig-cap: "A graphical example of collider bias. Both $X$ and $Y$ directly affect the collider $B$. (a) As long as $B$ is not conditioned on, the causal effect of $X$ on $Y$ is unbiased. (b) Conditioning on $B$ will introduce bias in the model. (c)  A simulation of the effect of $X$ on $Y$ in both models."

dag10 <- dagify(B ~ X + Y, Y ~ X)
coordinates(dag10) <- list(x = c(X = 0, B = 0.5, Y = 1), y = c(X = 1, B = 0, Y = 1))

p10.1 <- ggdag_classic(dag10, size = 15) + theme_dag_blank()
p10.2 <- ggdag_classic(dag10, size = 15) + theme_dag_blank() + annotate("text", label = "B", x = 0.5, y = 0, color = "grey80", size = 15) +  theme_dag_blank(plot.background = element_rect(color = "#B2423B", linewidth = 3))

v6 <- sim(coll)
v7 <- sim(coll, block = TRUE)

p10.3 <- ggplot() +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_vline(xintercept = 1, linewidth = 1.2) +
  geom_density(aes(x = v6), color = "royalblue", linewidth = 1.2) +
  geom_density(aes(x = v7), color = "darkred", linewidth = 1.2) +
  labs(x = "effect of X on Y", y = NULL) +
  xlim(c(-1,3)) +
  annotate("text", label = "true effect", x = 1.4, y = max(density(v6)$y)) +
  annotate("text", label = "biased effect (b)", x = -0.6, y = max(density(v6)$y) * 0.9, color = "darkred") +
  annotate("text", label = "estimated effect (a)", x = 1.8, y = max(density(v6)$y) * 0.5, color = "royalblue") +
  theme_minimal() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    axis.text.y = element_blank()
  )

(p10.1 + p10.2) / p10.3 + plot_annotation(tag_levels = "a")

```

## Conditioning Rules: The Backdoor Criterion

<!-- rationale behind the backdoor criterion -->

<!-- identification and rules -->
