---
title: ''
format: pdf
bibliography: ../references.bib
execute: 
  echo: false
editor:
  markdown:
    references: 
      location: block
      prefix: "theory"
---

```{r}
#| label: setup
#| warning: false
library(dagitty)
library(ggdag)
library(ggplot2)
library(patchwork)

# load simulation functions
source("../scripts/simulation.R")
```

# Theoretical Background

## Causality, Associations, and (In)dependence

In the preceding section, we defined causality as a concept involving hypothetical interventions. When intervening on a variable $X$ results in changes in another variable $Y$ we assert that $X$ causes $Y$. From a statistical standpoint, $X$ and $Y$ become dependent[^theory-1]. Conversely, an association only implies that $X$ and $Y$ share information; knowledge about one variable implies knowledge about the other variable, and *vice versa*. Crucially, associations lack directionality, whereas causality is typically understood as directional[^theory-2]. Causality can be one reason for associations to arise, but other reasons for associations exist, for example a shared common cause. Consequently, both causal relations and spurious relations can produce associations and render variables dependent. It is the underlying causal model that distinguishes between mere associations and causal relationships.

[^theory-1]: For the mathematical notation of (conditional) independence, see the @sec-prob.

[^theory-2]: There are of cause examples where causality can be bidirectional. For instance, in feedback loops, such as the price and demand models in economics, changes in price cause changes in demand and the other way around. But even in this case one can argue that these are essentially two different paths of causality that occur sequentially if observed with enough precision. For this thesis, we will not deal with feedback systems but stick with simpler models that assume purely directional causality.

## Graphical Causal Models

Graphical models provide a straightforward framework for conceptualizing causal systems. Pioneered by @pearl1995, they offer a visual representation of causal relationships, which eases development and comprehension of causal models. A graphical causal model visualizes the exposure, outcome, covariates, and their (assumed) causal relationships. In the following, we will typically denote the exposure[^theory-3] as $X$, the outcome as $Y$, and covariates[^theory-4] with other letters. Variables in a graphical causal model are linked by arrows. An arrow between $X$ and $Y$ means that a direct causal relationship between the two is possible (see @fig-dag1) . The direction of the arrow indicates the direction of causality. As depicted in @fig-dag1, $X \rightarrow Y$ means that $X$ causes $Y$ (and not the other way around). In accordance with our definition of causality, this implies that intervening on $X$ should result in a change in $Y$.

[^theory-3]: Exposure here is the medical term for what is often referred to as the "independent variable" in a statistical model. It is the variable that we imagine our intervention on, so it does not need to be an actual *exposure* in the strict sense of the word.

[^theory-4]: In this thesis, I use the term covariate for all variables related to the research question that are neither exposure nor outcome variables.

The direction of causality has to be determined by theoretical knowledge; it cannot be found in the data alone. Suppose that in our first example in @fig-dag1, $X$ represents biological sex and $Y$ denotes endurance performance. It seems apparent that a causal relationship exists between them (though it is undoubtedly much more complicated than that depicted in this simple model). However, the fact that it is sex that causes performance – and not the other way around – is based purely on theoretical knowledge and understanding of the world. There are no controlled interventions possible (because you cannot easily intervene on a person's biological sex). Ultimately, the direction of causality is an assumption by the researcher.

```{r}
#| label: fig-dag1
#| out-width: '50%'
#| fig-scap: "A simple graphical causal model with two variables."
#| fig-cap: "A simple graphical causal model with two variables. The variable $X$ (exposure) is assumed to cause the variable $Y$ (outcome). No other variables are assumed to influence this process."

dag1 <- dagify(Y ~ X)
coordinates(dag1) <- list(x = c(X = 0, Y = 1), y = c(X = 0, Y = 0))
ggdag_classic(dag1, size = 15) + theme_dag_blank()

```

Causal systems in the world are typically more complex than consisting of only exposure and outcome, and thus the graphical causal models depicting them are more complex as well. A slightly more complex graph is displayed in @fig-dag2. $X$ and $Y$ are not directly linked anymore, but are connected indirectly via $B$. This sequence $X \rightarrow B \rightarrow Y$ is called a *causal path*. We will later see that some models also have non-causal paths.

```{r}
#| label: fig-dag2
#| out-width: '50%'
#| fig-scap: "A more complex graphical causal model that includes four variables."
#| fig-cap: "A more complex graphical causal model that includes four variables. $X$ and $A$ both cause $B$, which in turn causes $Y$."

dag2 <- dagify(Y ~ B, B ~ X + A)
coordinates(dag2) <- list(x = c(X = 0, A = 0, B = 0.5, Y = 1), y = c(X = 1, A = 0, B = 0.5, Y = 0.5))
ggdag_classic(dag2, size = 15) + theme_dag_blank()

```

The graph in @fig-dag2 is called a directed acyclic graph (DAG). It is directed, because all paths have arrows, which establish the direction of causality. It is acyclic, because there are no circular paths in it. Finally, it is a graph. All graphs in this thesis will be DAGs, as many of the concepts presented herein require this, and most research problems can be adequately formulated using them. More important than the arrows a DAG contains is which arrows are absent. A DAG should depict all *potential* causal relations relevant to the research question. If two variables are not connected, we explicitly assume that they do not causally relate to each other[^theory-5]. For example, in @fig-dag2, there is no direct link between $X$ and $A$, or between $X$ and $Y$.

[^theory-5]: In other words, if two variables are connected they may or may not have a causal relation. If two variables are not connected, we assume that they definitely have no causal relation. This is a strong assumption in many scenarios, but when reasoned properly, it forms the foundation of causal inference.

DAGs tell a story. For example, we can assign the variables in @fig-dag2 to a simple model of endurance performance. Let $X$ be biological sex, $A$ the nutritional status, $B$ the physiological capacity to perform endurance tasks, and $Y$ the endurance performance in a competition. Our model assumes that sex and nutrition both directly affect the physiological capacity, which subsequently affects performance. Conversely, it assumes that sex and nutrition are not causally related, and that neither directly affects performance; rather, their effect are indirectly mediated through physiological capacity[^theory-6].

[^theory-6]: A mediator is an intermediate variable in a causal path. For example, in the DAG $X \rightarrow B \rightarrow Y$, $B$ is a mediator as the causal effect of $X$ on $Y$ passes through it. A moderator, in contrast, is a variable that modifies an existing causal effect, either due to an interaction or because the causal effect is heterogeneous and varies depending on the level of the moderator variable [@rohrer2022; @vanderweele2009]

## Modeling Causal Systems & Error Terms

DAGs serve as an abstract concept to describe research problems. This level of abstraction allows one to plan a study and its data analysis on a conceptual level. However, for the actual data analysis or demonstration purposes, a DAG has to be filled with data and functions. One way to fill a DAG is to think of it as a linear regression model (or more precisely, as a linear structural equation model[^theory-7]). For instance, the simplest DAG in the form $X \rightarrow Y$ can be analyzed as the linear regression model $Y \sim X + \epsilon$. This assumes that $Y$ is an additive linear combination of other variables. In this thesis, we will view all DAGs as linear models, keeping in mind that other types of models (e.g., non-linear relationships, interactions) are possible. A special role in these linear models plays the error term $\epsilon$.

[^theory-7]: A linear structural equation model (SEM) is essentially a linear regression model with additional causal assumptions [@bollen2013]. All DAGs (and many of the research questions from the potential outcome framework of causal inference) can be rewritten as a linear SEM, assuming the additional constraints of linearity and additive components, although SEMs can theoretically also be generalized to a non-linear setting [@bollen2013]. The analysis of DAGs via linear SEM can bring insights into causal systems [e.g., @ding2015].

If we knew the true causal model and could measure all variables perfectly, we could perfectly determine all causal effects. In reality, this is impossible. One of the main reasons for this is the presence of unobserved factors (errors) that influence our relevant variables in the model. These errors can include factors like random measurement error or biological variability. Furthermore, since we can only investigate causal effects in a sample of the population, our research will only result in an estimate of the true causal effect we seek to determine (the estimand).

Just like in any statistical analysis, we aim to obtain unbiased and precise estimates. Unbiasedness means that on average, our estimate will correspond to the true value of the estimand. Precision means that the estimate should have a small variance, or in other words, that repeated measurements will yield similar estimates. Random error terms add imprecision, but not bias, to our model. We will later encounter scenarios that introduce bias. Precision in causal effect estimates is higher in simpler models. This is primarily because simpler models have fewer random error terms. Along a causal path, information is typically lost, even if the causal effects remain unchanged. This loss of information is caused by the additional error terms of intermediate variables. Longer causal paths, therefore, introduce uncertainty into causal effect estimates but do not induce bias.

For an example from sport science, consider two different causal effects. First, the effect of a running intervention on mitochondrial density. Second, the effect of a running intervention on endurance performance. Even if we assume that the effect in the second case is entirely mediated trough mitochondrial density (i.e., $intervention \rightarrow density \rightarrow performance$), the effect on endurance performance is harder to estimate. The primary reason is that endurance performance will be influenced by additional unobserved factors that do not influence mitochondrial density, such as motivation, pacing, or day-to-day variability.

Examining the causal model in @fig-error, we have to reconsider that the arrows drawn in a DAG are just as noteworthy as the arrows not drawn. In this example, both unobserved error terms are parent nodes, meaning that they are not influenced by any other relevant variable, including one other. This is a general assumption regarding unobserved error terms: We assume random errors to be uncorrelated. As soon as errors influence each other (directly or via other variables), we should explicitly model them[^theory-8].

[^theory-8]: The assumption of uncorrelated error terms is also common in applied statistics outside of causal inference. If error terms are correlated, this complicates the estimation of effects. We can model correlated error terms in a DAG by creating a node for an unobserved variable. Another way to investigate the consequences of correlated error terms in linear SEMs is by drawing them from a multivariate normal distribution with an appropriate covariance matrix [e.g. in @ding2015].

```{r}
#| label: fig-error
#| out-width: '50%'
#| fig-scap: "A simple causal path with random error."
#| fig-cap: "A simple causal path with random error. (a) $X$ causes $Y$, but both variables are influenced by unobserved variables (random error). This adds imprecision to our model estimates, but on average, the true effect will be estimated (i.e., the model is unbiased)."

dag3 <- dagify(Y ~ X + U2, X ~ U1)
coordinates(dag3) <- list(x = c(X = 0, Y = 1, U1 = 0, U2 = 1), y = c(X = 0, Y = 0, U1 = 1, U2 = 1))
p3.1 <- ggdag_classic(dag3, size = 15) + theme_dag_blank()

p3.1

```

## Conditioning

Causal paths can be blocked by conditioning on intermediate variables. Take the causal path $X \rightarrow A \rightarrow Y$ as an example. Let $X$ be the stroke volume of the heart, $A$ the maximum oxygen uptake, and $Y$ the endurance performance in a competition. We assume that all of the causal effect of stroke volume on endurance performance is mediated via maximum oxygen uptake. However, if we condition on maximum oxygen uptake, no relationship between stroke volume and endurance performance remains. Conditioning on the intermediate variable $A$ effectively blocks the causal path between $X$ and $Y$, rendering the causal effect of stroke volume on endurance performance non-existing.

Several ways to condition on variables exist[^theory-9]. An experimental approach is to stratify the sample by the variable. For instance, if we would only investigate athletes with a similar maximum oxygen uptake, we would anticipate that the relationship between stroke volume and endurance performance would diminish. A modeling approach of conditioning on a variable is to include it in the statistical model. For example, modeling $Y \sim A + X + \epsilon$ would effectively block the causal effect of $X$ on $Y$ (see @fig-block)[^theory-10].

[^theory-9]: The mathematical notation of conditioning is straightforward (see @sec-prob). The exact methods for conditioning are diverse and include methods that can be applied during experimental design or data analysis.

[^theory-10]: Another popular way of conditioning is covariate-balancing [@stuart2010], which is discussed later in @sec-balancing.

```{r}
#| label: fig-block
#| out-width: '50%'
#| fig-scap: "A causal path blocked by conditioning."
#| fig-cap: "A causal path blocked by conditioning. The causal path is blocked, because the analysis conditions on $A$. Since all effects of $X$ on $Y$ pass through $A$, conditioning on $A$ means that no causal effect remains."

dag8 <- dagify(A ~ X, Y ~ A)
coordinates(dag8) <- list(x = c(X = 0, A = 0.5, Y = 1), y = c(X = 0, A = 0, Y = 0))
p8.2 <- ggdag_classic(dag8, size = 15) + theme_dag_blank() + annotate("text", label = "A", x = 0.5, y = 0, color = "grey80", size = 15)

p8.2

```

One of the main goals of causal inference using graph-based methods is identification — to identify which variables should be conditioned on to obtain estimates of causal effects. This process is crucial for providing unbiased and precise effect estimates. Depending on the model's structure, certain variables can introduce bias if not conditioned on, while others introduce bias if conditioned on. The following section will further elucidate these concepts by introducing confounders and colliders.

## Confounders and Colliders

Confounders are variables that causally influence the exposure and the outcome (see @fig-confcoll a). The confounder creates a spurious (non-causal) association between both variables. Conceptually, a confounder provides a set of similar knowledge to both exposure and outcome. This leads to them sharing common information, regardless of their true causal relationship, resulting in bias in the causal effect estimate.

Confounders can be controlled for by conditioning on them in the model. This removes the entire bias and preserves the true causal relationship. Let's take an example from running. We are interested in the relationship between the (average) 5000-m time trial speed and the (average) 100-m sprinting speed. We assume that being fast in an endurance task reduces the ability to sprint quickly, and thus decreases the 100-m speed. Therefore, we are interested in the causal relationship between $X$ (endurance speed) and $Y$ (sprinting speed). Note that this is a very simplistic causal model, as we could also model the unobserved ability to sprint and ability to perform endurance tasks, as well as their potential causes. Our model includes a confounder $A$, representing biological sex (see @fig-confcoll a). Based on expert knowledge, we understand that sex causally influences both sprinting and endurance performance, mainly via anthropometry and physiology. As a result, sex biases the causal relationship between sprinting and endurance performance. To remove this bias, the analysis must control for sex. For a discrete variable like sex is typically documented as, controlling for means in practice stratifying the analysis by it. Assuming our causal model is correct — which, of course, is not true in this simplified example here — controlling for sex gives us the true (unbiased) causal relationship between endurance and sprinting performance.

```{r}
#| label: fig-confcoll
#| out-width: '50%'
#| fig-scap: "Confounders and colliders in a directed acyclic graph."
#| fig-cap: "Confounders and colliders in a directed acyclic graph. (a) A graphical example of confounding. Both $X$ and $Y$ share a common cause $A$. The confounder introduces bias in determining the causal effect of $X$ on $Y$. Conditioning on $A$ removes this bias in the analysis. (b) A graphical example of collider bias. Both $X$ and $Y$ directly affect the collider $B$. As long as $B$ is not conditioned on, the causal effect of $X$ on $Y$ remains unbiased. However, conditioning on $B$ will introduce bias into the analysis."

dag9 <- dagify(X ~ A, Y ~ A + X)
coordinates(dag9) <- list(x = c(X = 0, A = 0.5, Y = 1), y = c(X = 0, A = 1, Y = 0))
p9.1 <- ggdag_classic(dag9, size = 15) + theme_dag_blank()

dag10 <- dagify(B ~ X + Y, Y ~ X)
coordinates(dag10) <- list(x = c(X = 0, B = 0.5, Y = 1), y = c(X = 1, B = 0, Y = 1))
p10.1 <- ggdag_classic(dag10, size = 15) + theme_dag_blank()

p9.1 + p10.1 + plot_annotation(tag_levels = "a") & theme(plot.tag = element_text(size = 22))

```

Colliders pose a more subtle form of bias. A collider is a variable causally influenced by both the exposure and the outcome (see @fig-confcoll b). Colliders themselves do not inherently cause harm. But conditioning on them introduces bias into a model[^theory-11]. This collider bias arises because a collider integrates information from both its source, the exposure and the outcome, and thus also of their causal relationship. If we condition on a collider we remove some of this integrated information, which can obscure the true causal relationship between the exposure and the outcome. If the exposure $X$ represents an experimental treatment that causes the collider $B$, this means that $B$ must be a post-treatment variable. Conditioning on this collider can introduce bias into causal effect estimate, not only in observational data but also in experimental research. This is why researchers should generally avoid conditioning on post-treatment variables in their analyses [@montgomery2018].

[^theory-11]: Equally, conditioning on a descendant of a collider introduces bias (though generally not as large as when conditioning on the collider itself).

As an example of collider bias, consider the causal relationship between $X$ as the post-test lactate concentration in a ramp test and $Y$ as the maximum oxygen uptake in the same ramp test. Our question is whether a higher lactate concentration causes a different (higher or lower) maximum oxygen uptake. In our model, both lactate concentration and maximum oxygen uptake influence the maximum speed achieved in the ramp test. This is reasonable because individuals with superior glycolytic and oxidative energy metabolism are likely to outperform their counterparts that have neither in terms of the maximum speed. The maximum speed attained thus acts as the collider $B$ in this scenario (see @fig-confcoll b). Conditioning on it will introduce bias into our model.

## Conditioning Rules: The Backdoor Criterion

Building on the concepts of confounders and colliders, we can derive more general rules for determining the optimal conditioning set for a given causal model. The most famous of these conditioning rules is the backdoor criterion [e.g., @pearl2009]. The backdoor criterion works by two steps: first, identifying all non-causal paths (backdoor paths), and second, blocking all of them. A non-causal path is any path between $X$ and $Y$ that starts with an arrow pointing into $X$. A non-causal path is open if it contains no collider or no variable conditioned on within it. It can be blocked (closed) by conditioning on a non-collider. For example, in @fig-confcoll a, $X \rightarrow Y$ is a causal path, while $X \leftarrow A \rightarrow Y$ is a non-causal path. The non-causal path can be blocked by conditioning on $A$, thus fulfilling the backdoor criterion and providing an unbiased estimate of the causal effect of $X$ on $Y$.

Non-causal paths are blocked by default if they contain a collider. For example, in @fig-blocked, the non-causal path $X \leftarrow A \rightarrow B \leftarrow Y$ is blocked by default because $B$ is a collider. Consequently, the backdoor criterion is satisfied and no conditioning is required. However, if one were to condition on $B$ in this scenario (for example if $A$ were unobserved, and we decided to condition on all observed covariates), this would reopen the backdoor path and introduce bias into the estimate.

```{r}
#| label: fig-blocked
#| fig-height: 3
#| out-width: '50%'
#| fig-scap: "A graphical example of a backdoor path closed by default."
#| fig-cap: "A graphical example of a backdoor path closed by default. The non-causal path via $A$ and $B$ contains a collider and is therefore closed. Conditioning on $B$ would reopen the backdoor path."

dag_blocked <- dagify(X ~ A, B ~ A + Y, Y ~ X)
coordinates(dag_blocked) <- list(x = c(X = 0, A = 0, B = 1, Y = 1), y = c(X = 0, A = 1, B = 1, Y = 0))

p_blocked <- ggdag_classic(dag_blocked, size = 15) + theme_dag_blank()
p_blocked

```

The backdoor criterion helps to determine which variables need to be conditioned on in graphical causal models of various complexities to obtain an unbiased estimate. These variables form the so-called minimal sufficient conditioning set. For example, in @fig-blocked, no conditioning is needed, and thus the minimal sufficient conditioning set is empty. Conditioning on more variables than necessary can increase precision in some cases but can also introduce the risk of new bias or reduced precision. When certain variables in a DAG are unobserved, they cannot be conditioned on. In such cases it may be impossible to find a minimal sufficient conditioning set that satisfies the backdoor criterion. Consequently, unbiased estimation of the causal effect, given the assumed causal model, becomes impossible.
