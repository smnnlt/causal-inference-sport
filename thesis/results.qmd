---
title: ''
format: pdf
bibliography: ../references.bib
editor:
  markdown:
    references: 
      location: block
      prefix: "results"
---

# Results {#sec-results}

## Example 1: Use of Different Running Shoes and Injury Risk {#sec-example1}

In running, overuse injuries occur frequently [@lopes2012]. As the feet transmit all ground forces, running shoes have been the focus of many injury prevention strategies [@sun2020]. A common belief in running practice is that the parallel use of different shoes increases movement variability and decreases injury risk, but the scientific evidence for this is limited [@vanmechelen1992].

@malisoux2015 tested the claim that concomitant use of running shoes decreases injury risks in an observational study. Using a prospective cohort design, they followed a group of 264 runners training for a marathon and documented their anthropometrics, training characteristics, shoe use, and injury occurrence. @malisoux2015 categorized runners into multi-shoe and single-shoe users, where multi-shoe users were those who reported changing running shoes at least twice between training sessions over the observation period. The authors fit several Cox proportional hazard regressions to the data[^results-1]. Using a semi-automated parameter selection, they finally arrived at a multivariate ("adjusted") model, with the coefficients indicating that multiple-shoe users had indeed a lower injury risk.

[^results-1]: Cox regression is a popular regression tool for survival analysis. In short, the survival rate over time depending on one or more covariates is modeled. In this case, being non-injured over a given amount of training volume was compared between the group of multiple-shoe and single-shoe users [@malisoux2015].

The study by @malisoux2015 was clearly causal in its aim. The title "Can parallel use of different running shoes decrease running-related injury risk?" poses a causal question. The hypothesis is of causal nature, the authors discuss "protective factors", speculate about potential causal mechanism of their findings, and state that multiple shoe use "could be advised to recreational runners to prevent running-related injuries" [@malisoux2015]. However, they acknowledge the low statistical power of their study and suggest that larger and longer observational studies or randomized controlled trials should be conducted to confirm their findings. I will here revisit the study by @malisoux2015 from a causal inference perspective.

The inherent drawback of the observational study by @malisoux2015 is the lack of randomization. In this study, the conditions of single or multiple shoe use are not randomly assigned to a runner, but are chosen by the runners themselves (though implicitly, as the research goal was not communicated in advance). When treatment conditions are chosen instead of randomly assigned, this can introduce bias in estimating the causal effects of the treatment. Essentially, in this case the assignment to a condition is likely not independent of the (projected) outcomes. Confounding variables may bias the causal relationship between treatment and outcome. An indicator of this might be the group imbalances in the pre-treatment variables seen in Table 1 of @malisoux2015. Multiple shoe users were, on average, older and had more regular training and racing in the year before the study. While some baseline imbalances are natural even with randomization, this pattern indicates that some variables potentially had an influence on the choice of using multiple running shoes, and these variables are confounders if they also have an effect on injury risk as the outcome variable.

A useful way to check for baseline imbalances in causal analyses is investigating propensity score overlap. The propensity score is the probability to be assigned to one of the treatment conditions conditional on the observed covariates, and is usually estimated with a logistic regression [@rosenbaum1983]. Comparison of propensity score distributions between the treatment groups can help diagnose covariate imbalances in a multivariate setting. In a completely randomized design, the propensity score distribution of treatment groups should be fairly similar. Differences in the propensity score distributions indicate a dependency between treatment assignment and covariates, potentially biasing the causal effect estimate. Assuming that the variables leading to this bias are all observed, we can use methods to correct for covariate imbalances[^results-2]. In this example, if we assume that certain variables influence the outcome of injury and the treatment variable of shoe use, we may compare only those single and multiple shoes users who share similar values for covariates. Methods for covariate balancing involve matching, reweighing, and subclassification procedures [@stuart2010]; these are discussed later in @sec-balancing.

[^results-2]: In other words, we assume that we can create independence between group assignment and outcome given the observed covariates (sometimes called the "ignorability assumption"). See @sec-math for the mathematical notation. Another assumption is that of common support. Despite covariate imbalances, some overlap between the covariate distributions of the treatment condition must exist. If this is not given (e.g., if the propensity score distributions between groups not only differ but are completely separated), we cannot reasonably balance the sample and estimate the causal effect because it would heavily rely on extrapolation. For the given example, this means that when multiple and single shoe users are almost totally different in their characteristics, we cannot adequately adjust the data to identify the true causal effect of shoe use.

```{r}
#| label: fig-dagshoes
#| out-width: '50%'
#| fig-scap: "A potential graphical model for the effect of shoe usage on running injuries."
#| fig-cap: 'A potential graphical model for the effect of shoe usage on running injuries. "perf" represents race performance. "char" stand for characteristics of the biological structures (e.g., bone mineral density).'

library(ggdag)
library(dagitty)

dag_shoes <- ggdag::dagify(training ~ experience,  risk ~ training + shoes + char, shoes ~ experience + training, perf ~ training + experience + age, experience ~ age, char ~ training + age, injury ~ risk)

coordinates(dag_shoes) <- list(x = c(shoes = 0.2, risk = 0.5, injury = 0.8, perf = 0.2, training = 0.5, char = 0.8, experience = 0.35, age = 0.65), y = c(shoes = 0.2, risk = 0.2, injury = 0.2, perf = 0.5, training = 0.5, char = 0.5, experience = 0.8, age = 0.8))

p_shoes <- ggdag::ggdag_classic(dag_shoes, size = 8) + ggdag::theme_dag_blank()
p_shoes
```

A potential DAG of the study is depicted in @fig-dagshoes. The occurrence of injuries depends on the volume of exposure and the injury risk; since exposure is controlled analytically by modeling injuries per hour of workload, it is omitted from this DAG for simplicity. In the causal model, injury risk has three potential causes: shoe usage, training characteristics unrelated to duration (e.g., intensity, elevation gain, running surface), and biological factors (e.g., bone mineral density). Notably, performance level is not listed as a direct cause of (increased or decreased) injury risk because there is no reason to believe that performance level itself impacts injuries directly (though it may indirectly as a proxy for experience, age, and training characteristics). While the DAG in @fig-dagshoes is far from perfect and can be debated, it provides a potential causal model with considerable complexity and several potential confounders. Assuming this DAG adequately represents the underlying causal network, a minimal sufficient conditioning set to satisfy the backdoor criterion would contain age and training characteristics. This set is challenging to condition on, as confounding training characteristics are difficult to measure comprehensively.

@malisoux2015 acknowledge the potential impact of confounders. Consequently, they do not directly interpret bivariate analyses of any variable with injury risk, but provide an "adjusted" multivariate model. This model is used to estimate the effect of parallel running shoe usage on injury risk while controlling for confounders. However, not only the coefficient of running shoe use but also the coefficients for other variables in the final model are interpreted causally (e.g., the effect of participation in sports other than running). Direct causal interpretation of multiple coefficients from multivariate models has been criticized as the "Table 2 fallacy" and is generally regarded as poor statistical practice [@westreich2013; @keele2020]. Moreover, in the absence of preregistration, this practice can lead researchers to present post-hoc hypothesis as though they were a priori [@kerr1998]. Even when focusing solely on the primary causal effect of interest, the final model in @malisoux2015 is likely to yield a biased estimate. The "adjusted" model was selected by first performing a bivariate screening of all available variables followed by an automatic selection procedure applied to a subset of these variables (with two variables manually included). Methodological literature generally advises against bivariate screening and finds automated variable selection highly debatable [@sun1996]. Current best practices suggest using background knowledge for variable selection [@heinze2018] and, if this is not sufficient, at least applying regularization methods [e.g., @fan2002].

Another potential source of bias lies within the definition of the treatment variable in @malisoux2015. Multiple shoe users are classified by having changed shoes at least twice during the observation period. This criterion might directly influence the outcome variable (non-injury over the observation period): On one hand, athletes who get injured subsequently drop out of the study, thus having less time to accumulate shoe changes and be categorized as multiple-shoe users. These athletes might have been classified as multiple-shoe users had they trained for a longer period instead of getting injured. On the other hand, those who voluntarily dropped out of the study were categorized as non-injured after a check. These athletes had less time to accumulate shoe changes and may therefore have been more likely to be characterized as single shoe users. Both scenarios create a non-causal relationship between the particular definition of shoe usage and injury risk. To address this potential source of bias and improve causal interpretation, providing information on observation duration could be beneficial, such as through a survival curve [@kaplan1958]. Additionally, directly modeling drop-out or testing the robustness of the model by using momentary rather than retrospective group assignment might offer statistical ways to handle these potential biases.

Taken together, from a causal inference perspective, the results by @malisoux2015 are subject to scrutiny. The study would benefit from a discussion of an underlying causal model (e.g., in the form of a DAG) and the application of statistical methods to address non-randomized group assignment (e.g., propensity score-based weighting). At a minimum, the definition of multiple shoe use should be reassessed, and survival curves should be included into the analysis. Additionally, the relatively small sample size (with a low absolute number of injuries) leads to imprecise estimates, even if the results were unbiased. Therefore, I concur with @malisoux2015 that either RCTs or larger observational studies should be conducted if the research question is deemed relevant. Moreover, employing appropriate causal inference methods could enhance the analysis in each case.

## Example 2: Nutrient Intake and Mountain Marathon Performance

In ultra-endurance races, adequate nutrient intake is crucial for both performance and health reasons [@nikolaidis2018; @costa2019; @williamson2016]. Athletes are advised to maintain appropriate fluid and carbohydrate intake troughout races [@thomas2016], which should in theory benefit performance. However, the effect of nutritional intake on ultra-endurance performance has been rarely investigated in field settings.

@kruseman2005 documented nutrient intake for 46 runners during an ultra-marathon mountain race. Additionally, they measured anthropometrics before and after the race and recorded race performance. The observational cohort study had the primary aim of providing descriptions of actual nutrition strategies during an ultra-endurance competition and compare them with published guidelines. The secondary aim by @kruseman2005 was to investigate "the association between nutrient intake and performance". To test their secondary aim, the authors devided the group into performance tertiles and tested bivariate relationships with anthropometric, running experience, and nutrient intake variables using analysis of variance and $\chi^2$-tests. They then selected the statistically significant variables from these bivariate analyses and applied a multivariate regression model with backward stepwise selection. @kruseman2005 found that most athletes failed to meet the nutrient recommendations for the race, but no significant association with performance was observed.

The secondary aim by @kruseman2005 is causal; it is based on the hypothesis that inadequate nutrient intake impairs performance. However, they acknowledge that "\[b\]eing a cross-sectional, observational study, no causal relationship can be drawn between \[nutrient\] intake and performance", which appears counterintuitive to their research goal. @kruseman2005 found significant associations between nutrient intake and performance in their bivariate analysis, but not in the multivariate model, which adjusted (among other factors) for previous race experience. The authors adopt a critical stance towards their own results and recommend further experimental studies[^results-3]. In the following section, I will revisit the study by @kruseman2005 from a causal inference perspective.

[^results-3]: The authors' critical discussion of the causality of their findings is surprising for sport science. Considering that they correctly identified the limitations of their data analysis, I question why the authors chose to conduct such an analysis in the first place. It would have been interesting to see how critical the authors would have been if their multivariate model had indeed revealed the predicted significant effect of nutrient intake on performance instead of a null result.

```{r}
#| label: fig-dagnutrition
#| out-width: '50%'
#| fig-scap: "A potential graphical model for nutrition intake in an ultra-marathon race."
#| fig-cap: 'A potential graphical model for nutrition intake in an ultra-marathon race. "plevel" represents performance level (the physiological capability to perform the endurance task). "perf" stands for race performance.'

dag_nutrition <- ggdag::dagify(training ~ experience,  plevel ~ training, intake ~ experience, perf ~ intake + plevel)
coordinates(dag_nutrition) <- list(x = c(experience = 0.5, training = 0.575, plevel = 0.65, intake = 0.35, perf = 0.65), y = c(experience = 0.65, training = 0.5, plevel = 0.35, intake = 0.2, perf = 0.2))

p_nutrition <- ggdag::ggdag_classic(dag_nutrition, size = 8) + ggdag::theme_dag_blank()
p_nutrition
```

A potential DAG for nutrient intake and ultra-endurance performance is depicted in @fig-dagnutrition. Nutrient intake directly affects performance; for instance, low carbohydrate availability and dehydration can cause fatigue and thus decrease performance. Nutrient intake is largely influenced by an athlete's experience (e.g., knowledge of nutritional strategies, prior race experience). Experience also affects an athlete's training, both qualitative (e.g., experienced athletes may better tailor their training) and quantitative (e.g., more experienced athletes have had more time in their life to accumulate running training). In turn, training influences the performance level, the physiological capacity to perform the given endurance task before the race begins. This performance level, combined with nutrient intake during the race, ultimately determines the final race performance.

If the DAG in @fig-dagnutrition adequately represents the causal model underlying @kruseman2005, then the effect of nutrient intake on performance is biased due to an open backdoor path. This backdoor path could be closed by conditioning on any intermediate variable. Since experience is the only of the three intermediate variables of @fig-dagnutrition that was measured by @kruseman2005, it seams reasonable to condition the analysis on this variable[^results-4]. @kruseman2005 acknowledge that experience is a confounder in the causal relationship between nutrient intake and performance, noting: "Because experienced runners are well trained, fitter, and know their personal needs better during such a race, it is impossible to precisely separate the associations we found, especially in a cross-sectional design." But given the DAG in @fig-dagnutrition, conditioning on experience in the statistical model does indeed allow to "separate" the causal effect of nutrient intake on performance.

[^results-4]: Conditioning on experience, rather than on training, offers additional benefits if we modify the model to allow a direct effect of experience on performance that is not mediated by training. For example, psychological readiness and pacing strategy could both influence performance and may be influenced more by experience than by training.

@kruseman2005 close the backdoor path by conditioning on experience in their multivariate model, though this was likely done rather inadvertently as their variable selection procedure was driven by automated rules rather than background knowledge. The resulting conditional effect of nutrient intake on performance is non-significant[^results-5]. According to the DAG in @fig-dagnutrition, this should be a less biased estimate compared to the bivariate associations between nutrient intake and performance that @kruseman2005 also report, but caution as potentially spurious. Interestingly, @kruseman2005 disregards both the unadjusted and adjusted estimates as biased, stating that neither "does allow us to conclude any definitive causal relationships". This is of course true for any effect estimate, particularly in the context of observational studies. But given the futility of chasing "definitive causal relationships", it should be the aim of any researcher to minimize bias in causal effect estimates to ensure that their analysis remains meaningful at all.

[^results-5]: Technically we do not know from @kruseman2005 if the effect estimate is non-significant. We just know that the variables related to nutrient intake were removed from a model by backward stepwise selection. As the selection criteria was probably statistical significance, it is likely that nutrient intake variables would also have been non-significant in a separate model only conditioned on experience. But again, we do not actually know this because of the automated variable selection model used.

A modified DAG can address some of the skepticism expressed by @kruseman2005 regarding their effect estimate adjusted for experience. If we introduce a further causal relationships from training to nutrient intake in @fig-dagnutrition , this creates an additional background path that cannot be closed by merely conditioning on experience. @kruseman2005 hint at such a relation by noting that "in addition, training increases the benefits of adequate nutrition, and favors the accumulation of muscle glycogen after exercise". This suggests that training might act as a moderator of the relationship between nutrient intake and performance. Since previous training was not documented in their study, it could not be controlled for, leaving this backdoor path open and potentially introducing confounder bias into the causal effect estimate. Again – though not in the language of causal inference – @kruseman2005 acknowledge this by stating: "It would have been interesting to record the training level of the participants and study the potential confounding effect of training level on nutritional intake during a race. However, race experience seems an adequate indirect marker of training, as is body fat mass." They are right that experience can serve as an indirect marker of training given the DAG in @fig-dagnutrition, but assuming a direct effect of training on nutrient intake the statement does not hold true. Presenting and discussing potential causal models would have reasonably benefited the analysis of @kruseman2005.

Even without a DAG @kruseman2005 discuss their results in light of potential causal relationships between variables. But their data analysis does not include this causal knowledge, relying solely on automated variable selection procedures. Both bivariate screening and backward stepwise selection are generally inadvisable for causal inference [@sun1996]. Additionally, with a sample size of 46 athletes and substantial heterogeneity in covariates, obtaining precise effect estimates may be challenging. An alternative experimental approach could involve analyzing performance across different sections of the ultra-marathon race. Nutrient intake likely has varying importance depending on the race segment; for instance, it may not affect performance in the initial hour of the race. Examining the causal effect of nutrient intake by segmenting race performance could yield a better causal effect estimate and provide an additional plausibility check for the model.
