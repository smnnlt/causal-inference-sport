---
title: ''
format: pdf
bibliography: ../references.bib
---

# Results

## Example 1: Use of Different Running Shoes and Injury Risk

In running, overuse injuries occur frequently [@lopes2012]. As the feet transmit all ground forces, running shoes have been the focus of many injury prevention strategies [@sun2020]. A common belief coming from running practice is that parallel use of different shoes increases movement variability and decreases injury risk, but the scientific evidence for this is limited [@vanmechelen1992].

@malisoux2015 tested the claim that concomitant use of running shoes decreases injury risks in an observational study. Using a prospective cohort design, they followed a group of 264 runners training for a marathon and documented their anthropometrics, training characteristics, shoe use, and injury occurrence. @malisoux2015 categorized runners into multi-shoe and single-shoe users, where multi-shoe users were those who reported to have changed running shoes at least twice between training sessions over the observation period. The authors fit several Cox proportional hazard regressions to the data[^1]. Using a semi-automated parameter selection, they finally arrived at a multivariate ("adjusted") model, with the coefficients indicating that multiple-shoe users had indeed a lower injury risk.

[^1]: Cox regression is a popular regression tool for survival analysis. In short, the survival rate over time depending on one or more covariates is modeled. In this case, being non-injured over a given amount of training volume was compared between the group of multiple shoe and single shoe users [@malisoux2015].

The study by @malisoux2015 was clearly causal in its aim. The title 'Can parallel use of different running shoes decrease running-related injury risk?' poses a causal question, the hypothesis is of causal nature, the authors discuss 'protective factors', speculate about potential causal mechanism of their findings, and state that multiple shoe use "could be advised to recreational runners to prevent running-related injuries" [@malisoux2015]. However they acknowledge the low statistical power of their study and suggest that larger and longer observational studies, or randomized controlled trials should be conducted to confirm their findings. I will here revisit the study by @malisoux2015 from a causal inference perspective.

The inherent drawback of the observational study by @malisoux2015 is the lack of randomization. The conditions of single or multiple shoe use are not randomly assigned to a runner, but are chosen by them (though implicit, as the research goal was not communicated in advance). When treatment conditions are chosen instead of randomly assigned this can introduce bias when estimating causal effects of the treatment. Essentially, in this case the assignment to a condition is likely not independent of the (projected) outcomes. Confounders may bias the causal relationship between treatment and outcome. An indicator of this might be the group imbalances in the pre-treatment variables seen in Table 1 of @malisoux2015. Multiple shoe users were on average older and had more regular training and racing in the year before the study. While some baseline imbalances are natural even in the case of randomization, this pattern indicates that some variables potentially had an influence in the choice of using multiple running shoes, and these variables are confounders if they also have an effect on injury risk as the outcome variable.

A useful way to check for baseline imbalances would be investigating propensity score overlap. The propensity score is the probability to be assigned to one of the treatment conditions based conditional on the observed covariates, and is usually estimated with a logistic regression [@rosenbaum1983]. Comparison of propensity score distributions between the treatment group can diagnose covariate imbalances in a multivariate setting. In a completely randomized design, the propensity score distribution of treatment groups should be fairly similar. Differences in the propensity score distributions indicate a dependency between treatment assignment and covariates, potentially biasing the causal effect estimate. Assuming that the variables leading to this bias are all observed, we can use methods to correct for covariate imbalances[^2]. In this example, if we assume that certain variables influence the outcome of injury and the treatment variable of shoe use, we may compare only those single and multiple shoes users that share similar values for covariates. Methods for covariate balancing involve matching, reweighing, and subclassification procedures [@stuart2010], these are discussed later (LINK).

[^2]: In other words, we assume that we can create independence between group assignment and outcome conditional on the observed covariates (sometimes called the "ignorability assumption". See the APPENDIX for the mathematical notation. Another assumption is that of common support. Despite covariate imbalances, some overlap between the covariate distributions of the treatment condition needs to exist. If this is not given (e.g., the propensity score distributions between groups not only differ, but are completely separated), we cannot reasonably balance the sample and estimate the causal effect because it would heavily rely on extrapolation. For the given example this means, when multiple and single shoe users are almost totally different in their characteristics, we cannot adequately adjust the data to identify the true causal effect of shoe use.

A potential DAG is <!-- here. Add and discuss -->

@malisoux2015 are aware of the potential impact of confounders. This is why they do not directly interpret bivariate analyses of any variable with injury risk, but provide an "adjusted" multivariate model. This model is used to estimate the effect of parallel running shoe usage on injury risk, while controlling for confounders. However, not only the coefficient of running shoe use, but also the other coefficients of the final model are interpreted in a causal way (e.g., the participation in sports other than running). The direct causal interpretation of multiple coefficients from multivariate models has been called the "Table 2 fallacy" and it is generally regarded as bad statistical practice [@westreich2013]. Moreover when no preregistration was done, this practice may invite researchers to present post-hoc hypothesis as a priori [@kerr1998]. Even if only the primary causal effect of interest is considered, the final model by @malisoux2015 is likely to provide a biased estimate. The "adjusted" model is chosen by first performing a bivariate screening of all available variables and then an automatic selection procedure on a subset of these (with two variables manually included). In the methodological literature, bivariate screening is generally advised against, while automated variable selection is highly debatable [@sun1996]. Current best advice is to use background knowledge when selecting appropriate variables [@heinze2018] and if this is not sufficient at least use regularization methods [e.g., @fan2002].

Another potential source of bias is hidden in the definition of the treatment variable in @malisoux2015. Multiple shoe users are defined by a minimum number of two shoe changes over the observation period. There may be a direct dependence between this grouping criterion and the outcome variable (non-injuredness over the observation period): On the one hand, athletes who receive an injury subsequently drop out of the study and thus have less time to accumulate shoe changes for being categorized as multiple-shoe users. Potentially, athletes would be considered multiple-shoe users if they had trained for a longer time instead of receiving an injury. On the other hand, people who dropped out of the study were, after a check, considered as non-injured. These athletes had less time to accumulate shoe changes and may have been more likely to be characterized as single shoe users. In both ways a non-causal relationship between the particular definition of shoe use and injury may exist. A way to check this source of bias and aid the causal interpretation of the study would be to give information on the observation duration, possible in form of a survival curve [@kaplan1958]. Directly modeling drop-out or testing the robustness of the model by using the momentarily instead of the retrospective group assignment may be a statistical way to deal with these potential biases.

Taken together, from a causal inference viewpoint the results by @malisoux2015 should be questioned. The study could benefit from the discussion of an underlying causal model (e.g., in form of a DAG) and the use statistical methods to deal with non-randomized group assignment (e.g., propensity score-based weighting). At a minimum, the definition of multiple shoe use should be rechecked and survival curves should be included in the analysis. Finally, the rather small sample size (low absolute number of injuries occurred) will lead to imprecise estimates even if unbiasedness can be assumed. Therefore I agree with @malisoux2015, that either RCTs or larger observational studies should be conducted, if the research question is of enough relevance. I would just add that appropriate causal inference methods could help in all of these cases.

## Example 2: Nutrient intake and marathon mountain performance

In ultra-endurance races, appropriate nutrient intake is essential both for performance and health reasons [@nikolaidis2018; @costa2019; @williamson2016]. Athletes are encouraged to maintain proper fluid and carbohydrate intake during races [@thomas2016], which should in theory benefit performance. The influence of nutritional intake on performance in actual ultra-endurance races has however rarely been investigated.

@kruseman2005 documented nutrient intake during a ultra-marathon mountain race in 46 runners. Additionally, they measured anthropometrics before and after the race and registered the race performance. The observational cohort study had the primary aim of providing descriptions of actual nutrition strategies during an ultra-endurance competition and compare them with published guidelines. The secondary aim by @kruseman2005 was to study "the association between nutrient intake and performance". To test their secondary aim, the authors split the group into performance tertiles and tested bivariate relationships to anthropometric, running experience, and nutrient intake variables using analysis of variance and $\chi^2$-tests. They then took the statistically significant variables from the bivariate analyses and ran a multivariat regression model with backward stepwise selection. @kruseman2005 showed that most athletes failed to meet the nutrient recommendations for the race, but there was no significant association with performance.

The secondary aim by @kruseman2005 is causal; it is build on the hypothesis that inadequate nutrient intake hinders performance. Yet they acknowledge, that "\[b\]eing a cross-sectional, observational study, no causal relationship can be drawn between \[nutrient\] intake and performance", which seems counterintuitive to the research goal. @kruseman2005 found significant associations between nutrient intake and performance in their bivariate analysis, but not in the multivariate model, that adjusted (among others) for previous race experience. The authors take a quite critical stance towards their own results and advise for further experimental studies[^3]. In the following section I will revisit the study by @kruseman2005 from a causal inference perspective.

[^3]: The way the authors critically discuss the causility of their findings is bracing for sport science. Given the correctly identified limitations of the data analysis, I wonder why the authors chose to perform such an analysis in first place. It would have been interesting to see how critical the authors would have been if their multivariate model had indeed included the predicted significant effect of nutrient intake on performance.

```{r}
#| label: fig-dagnutrition
#| fig-height: 4
#| out-width: '50%'
#| fig-scap: "A potential graphical model for nutrition intake in an ultra-marathon."
#| fig-cap: "A potential graphical model for nutrition intake in an ultra-marathon. plevel stands for performance level (the physiological capability to perform the endurance task)."


dag_nutrition <- ggdag::dagify(training ~ experience,  plevel ~ training, intake ~ experience, performance ~ intake + plevel)
coordinates(dag_nutrition) <- list(x = c(experience = 0.35, training = 0.6, plevel = 0.5, intake = 0.35, performance = 0.65), y = c(experience = 0.7, training = 0.7, plevel = 0.5, intake = 0.2, performance = 0.2))

p_nutrition <- ggdag::ggdag_classic(dag_nutrition, size = 8) + ggdag::theme_dag_blank()
p_nutrition
```

A potential DAG of nutrient intake and ultra-endurance performance is shown in @fig-dagnutrition. Nutrient intake has a direct causal effect on performance, as low carbohydrate availability and dehydration induce fatigue and thus reduce performance. Nutrient intake is mainly determined by an athlete's experience (e.g., knowledge about nutritional strategies, prior race experience). Experience determines the training of an athlete, both qualitative (e.g., experienced athletes may know better which training is suited for them) and quantitative (e.g., athletes with more running experience have had more time in their life to accumulate running training). Training in term influences the performance level, the physiological ability to perform the given endurance task pre-start. This performance level together with the nutrient intake during the race determines the final race performance.

Given that the DAG in @fig-dagnutrition is an appropriate representation of the causal model underlying @kruseman2005, the effect of nutrient intake on performance is biased by an open backdoor path. This backdoor path could be closed by conditioning on any intermediate variable. As experience is the only of the three intermediate variables of @fig-dagnutrition measured by @kruseman2005, it seams reasonable to condition the analysis on it[^4]. @kruseman2005 recognize that experience is a confounder of the causal relationship between nutrient intake and performance, as they write: "Because experienced runners are well trained, fitter, and know their personal needs better during such a race, it is impossible to precisely separate the associations we found, especially in a cross-sectional design." But given the DAG in @fig-dagnutrition, conditioning on experience in the statistical model does indeed allow to "separate" the causal effect of nutrient intake on performance.

[^4]: Conditioning on experience instead of conditioning on training has additional benefits if me modify the model by allowing direct effect of experience on performance, that are not mediated via training. Two possible examples would include psychological readiness and pacing strategy. Both influence performance and are possible more caused by experience than by training.

@kruseman2005 close the backdoor path by conditioning on experience in their multivariate model (though rather inadvertently as their variable selection procedure is not determined by background knowledge but by automated rules). The resulting conditional effect of nutrient intake on performance is non-significant[^5]. Based on the DAG in @fig-dagnutrition, this should be a less biased estimate than the bivariate associations between nutrient intake and performance, that @kruseman2005 also report, but flag as potentially spurious. Interestingly, @kruseman2005 disregards both estimates (unadjusted and adjusted) as biases, stating that neither "does allow us to conclude any definitive causal relationships". This is of course true for any effect estimate, particularly in the context of observational studies. But it should be aim of any researcher to reduce bias in causal effect estimates, or otherwise the analysis would be of no value at all. 

[^5]: Technically we do not know from @kruseman2005 not know if the effect estimate is non-significant. We just know that the variables related to nutrient intake were removed from a model by backward stepwise selection. As the selection criteria was probably statistical significance, it is likely that nutrient intake variables would also have been non-significant in a separate model only conditioned on experience.

A modified DAG can elucidate some of the skepticism by @kruseman2005 regarding their effect estimate adjusted for experience. If we assume in @fig-dagnutrition a further causal relationships from training to nutrient intake, this would create another background path that cannot be closed by only conditioning on experience. @kruseman2005 hint to such a relation by writing that "in addition, training increases the benefits of adequate nutrition, and favors the accumulation of muscle glycogen after exercise". This suggests, that training acts as a moderator of the relationship between nutrient intake and performance. As previous training was not documented in the study, it cannot be controlled for in the model, thus this open backdoor path would remain open and the causal effect would be biased by a confounder. Again — though not in the language of causal inference — this is recognized by @kruseman2005, as they write: "It would have been interesting to record the training level of the participants and study the potential confounding effect of training level on nutritional intake during a race. However, race experience seems an adequate indirect marker of training, as is body fat mass." They are right that experience is an adequate marker of training given the DAG in @fig-dagnutrition, but assuming a direct cause of training on nutrient intake this is false. Presenting and discussing potential causal models would have provided a reasonable benefit to the analysis of @kruseman2005.

Even without a DAG @kruseman2005 discuss their results in light of potential causal relationships between variables. But their data analysis is blind to this causal knowledge, as it only uses automated procedures for variable selection in the models. Both the bivariate screening and the backward stepwise selection are methods that should in general not be used for causal inference [@sun1996]. But even with an model selection informed by background knowledge, the study sample size of 46 athletes with great heterogeneity in their covariates may be too small to get precise effect estimates. A different experimental approach would to include the performance over different sections of the ultra-marathon race in the analysis. In general we can assume, that in-race nutrient intake becomes more important later during the race, as it should not have any influence on performance in for example the first hour of racing. Investigating the causal effect of nutrient intake by dissecting in-race performance may provide both a better causal effect estimate and an additional plausibility check for the model.
