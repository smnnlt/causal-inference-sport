---
title: ''
format: pdf
bibliography: ../references.bib
editor:
  markdown:
    references: 
      location: block
      prefix: "discussion"
---

```{r}
#| label: setup2
#| echo: false
library(ggdag)
library(dagitty)
```

# Discussion

The aim of this thesis was to introduce methods of causal inference to sport science. After an introduction to causal thinking based on graphical models, I demonstrated with two research examples the utility of using a causal viewpoint in planing and analyzing observational data. I will here first discuss the general application of causal inference in sport science and present special causal inference methods and their potential utility in sport science. I then discuss limitations and perspectives of causal modeling in sport to finally derive a workflow of how to implement causal modeling into the sport research cycle.

## General Applications of Causal Inference in Sport Science

<!-- General discussion why and under which circumstances causal inference is useful -->

<!-- RCT often impossible. Especially in elite sports (hard to do experiments) or when it comes to explaining sport performance (no direct manipulation of predictors possible). -->

## Applicability of Special Causal Inference Methods in Sports {#sec-ident}

Apart from general principles of causal thinking and modeling based on graphical representations, a set of special causal inference methods has gained popularity in the past decades. Based on the potential outcome framework [@rubin1974], these became standard tools in the analysis of observational data especially in the field of economics [@athey2017], but also beyond. @angrist1999 called these set of causal inference tools "identification methods", as they help to identify causal effect estimates in certain common situations. The identification methods are well-researched analysis tools, that may also prove helpful in many applications in sport science. I will here introduce five common identification methods and discuss their potential application in sport science[^discussion-1]. @tbl-ident provides a summary of the five methods.

[^discussion-1]: I chose the set of five methods based on @angrist1999 while making some modifications. I replaced the general "conditioning in a regression model" strategy, which has been discussed earlier, with covariate balancing methods, and added the newer method of synthetic control [both to some extent inspired by @athey2017; @cunningham2021].

<!-- overview in a table (landscape mode) -->

```{=tex}
\newpage
\KOMAoptions{paper=landscape,pagesize}
\recalctypearea
```
```{r}
#| echo: false
#| label: tbl-ident
#| tbl-cap: "A summary of causal identification methods and their application to sport science."
#| message: false

d <- data.frame(
  names = c("Covariate Balancing", "Instrumental Variables", "Regression Discontinuity", "Difference-in-Difference", "Synthetic Control"),
  idea = c("Creating groups balanced on observed covariates when group assignment was not random","Control for unobserved confounders by using a variable that only relates to the outcome via the treatment.", "Finding a treatment that is assigned based on a certain threshold, to compare individuals slightly above and below this threshold","Observing a quasi-experimental treatment and control group over time to estimate treatment effects", "Comparing a single time-series  to a synthetic control time-series based on imperfect control groups"),
  source = c("@stuart2010", "@greenland2000", "@imbens2008", "@lechner2011", "@abadie2021"),
  appl = c("Genetic profiling, injury research, team sport analytics", "Non-compliance and measurement errors in sport interventions, talent development", "Effects of winning and relegation, draft systems, squad nominations", "Rule changes, technological developments", "Coach changes, talent development programs")
)

flextable::qflextable(d) |>
  flextable::set_header_labels(values = c("Method", "Basic Idea", "Reference", "Applications in Sport Science")) |>
  flextable::width(width = c(2, 3, 1.5, 1.5)) |>
  ftExtra::colformat_md()

```

```{=tex}
\newpage
\KOMAoptions{paper=portrait,pagesize}
\recalctypearea
```
### Covariate Balancing {#sec-balancing}

A common approach to causal inference of observational data is to mimic the characteristics of a RCT. In a RCT, treatment assignment is random, and thus treatment groups only differ in their covariates by chance. Conversely, in observational studies covariates may influence treatment assignment. For example in the study of multiple running shoe use and injury risk from the previous  @sec-example1 [@malisoux2015], runners may decide if they use different shoes based on weekly running volume. If weekly running volume also influences the outcome parameter of injury risk, this is an classical example of confounder bias (see @fig-conf). To mimic a RCT of multiple running shoe use, we could decide to only compare individuals with similar running volume (and other covariates). This is the basic idea of covariate balancing.

Covariate balancing can broadly be defined in three categories: subclassification, matching, and reweighting [@stuart2010]. Subclassification groups individuals with similar covariates into subclasses, then compares different treatments only within the subclasses, and finally calculates a (weighted) average of these comparisons [@cochran1968]. Matching aims to find individuals with equal or similar covariates and compare only them, often on a 1:1 basis, before pooling all comparisons [@rubin1973]. This often involves discarding data for which no (sufficient) matches could be found. Reweighting keeps all observation, but gives them new weights based on how representative they are for their group.

Regardless of the method used, covariate balancing is typically a pre-analysis routine, i.e., it happens in a step before the actual causal data analysis and without including information on the outcome values. In some sense, it aims to solve the same problem of observed confounders that simple conditioning in a regression does address. Current advise is to use covariate balancing not instead of regression adjustments, but complementary, for example in the so called "doubly-robust" methods [@bang2005]. A benefit of covariate balancing over regression adjustments is that it eases the checking of overlap in covariate distributions. If this overlap is not given, linear adjustments tend to perform bad as they have to rely on extrapolation, but typical modelling workflows of linear regression do not involve simple checks of this.

A crucial question in covariate balancing is what defines "closeness" or similarity of covariates. Exact equality on all covariates will only work for large samples with few discrete covariates. In most cases researchers have to compute distance measures. One of the most common measures is the propensity score, the conditional probability that an individual was assigned to the treatment group given its covariates [@rosenbaum1983; @dehejia2002]. The propensity score thus reduces the multidimensional covariates to a single value. This value can be used to form subclasses, match units, or weight observations. The exact choice of a balancing method and a distance measure should be context-specific and the scientific debate which procedure works best is vital[^discussion-2]. @stuart2010 provides general recommendations regarding the choice of procedures. In general, it can be helpful to compare different methods (and method parameters) in a given data set.

[^discussion-2]: Just as a short glimpse into the debate: @frölich2004 argues that weighting is always worse than matching, a finding that is challenged by @busso2014. @iacus2011 heavily criticize the widely used propensity-score matching [@dehejia2002]. They instead propose their own method of coarsed exact matching [@iacus2011; @iacus2012], which has in turn received opposition by @black2020.

The first researchers have begin to adopt covariate balancing methods into sport science. In the field of injury rehabilitation, propensity score matching was used to find adequate control groups for athletes undergoing recovery [@farinelli2023; @owens2022; @fenn2023; @jimenez2022]. Others compared fitness levels in soccer players over different decades [@gonaus2023] and estimated the effect of playing venue of match outcome [@kneafsey2018] using different matching methods. @nakahara2023 used propensity-score based subclassification to evaluate pitching strategy in baseball. In an innovative article, @gibbs2022 used matching to investigate the effect of timeouts on stopping point runs in basketball games.

In general, covariate balancing can help in any situation in sport science, where we have a limited set of intervention, a set of observed pre-treatment variable, and rather large sample sizes. While covariate balancing methods can reduce bias, matching effectively discards observation units, and matching estimators are unstable in small sample sizes. Therefore, these methods are rather suited when there is at least a rather large control group of individuals. This could, for example, be comparing physiological markers of injured vs. uninjured athletes, or comparing genetic profiles of elite athletes against a non-elite population. An additional field of application could be non-randomized studies of health benefits from recreational sport participation. Covariate balancing can also help in understanding causal mechanisms of team sport performance [e.g., @gibbs2022].

### Instrumental Variables

```{r}
#| label: fig-instr
#| fig-height: 4
#| out-width: '50%'
#| fig-scap: "A graphical example of an instrumental variable."
#| fig-cap: "A graphical example of an instrumental variable. The relationship between exposure $X$ and outcome $Y$ is confounded by a set of unobserved variables $U$. The instrumental variable $Z$, which is unconfounded and only affects $Y$ via $X$, can be used to provide an unbiased estimate of the causal effect of $X$ on $Y$. "

dag_instr <- dagify(X ~ Z + U, Y ~ X + U)
coordinates(dag_instr) <- list(x = c(Z = 0, X = 0.5, U = 0.75, Y = 1), y = c(Z = 0, X = 0, U = 0.2, Y = 0))

p_instr <- ggdag_classic(dag_instr, size = 15) + theme_dag_blank()
p_instr
```

Confounders between received treatment and outcome are often not observed, or even known. In situation it is not directly possible to estimate an unbiased causal treatment effect. A way to reduce bias is by using an instrumental variable. Instrumental variables are variables, that cause the outcome only mediated by the treatment. For a graphical representation see @fig-instr. A set of unobserved variables $U$ influences both treatment $X$ and outcome $Y$, thus their causal relationship ist biased, as we cannot control for $U$ because it is unobserved. If we instead use the instrument $Z$, which only causes $X$ directly and $Y$ indirectly via $X$, we can isolate a part of the effect of $X$ on $Y$ that is not influenced by $U$.

A classic example of an instrument is random treatment assignment [@greenland2000]. People may actively decide if they want to receive a treatment, and these decisions may be driven by unobserved confounders that also cause the outcome variable. The assignment to a treatment does not directly influence the outcome, but only receiving a treatment does. As people assigned to the treatment group are much more likely to follow the assignment and thus receiving treatment, treatment assignment works as an instrument. Using an appropriate procedure (typically a two-stage least square estimator), researchers can estimate the causal effect of receiving a treatment on the outcome[^discussion-3].

[^discussion-3]: Depending on whether we assume heterogeneity in the treatment effect, the causal effect estimated is somewhat limited in its definition. Strictly speaking we only estimate the causal effect of treatment in those that adhere to treatment assignment. This is often called the locale average treatment effect of compliers. If we are interested in the mechanistic causes of receiving a treatment this should be what interests us. If the goal is to evaluate the causes of implementing a treatment on a population level (e.g., for policy research) it is more appropriate to include non-compliance in the estimation, and therefore not use instrumental variables.

Instrumental variables can be used to adjust for unobserved confounders and is helpful in problems of measurement error and non-compliance. As an example from sport science, @ruseski2014 investigated the causal effect of sport participation on happiness, a relationship that is likely confounded by unobserved variables (e.g., biography and socio-economic background). They used physical distance to the nearest sporting facility and personal belief in the benefits of exercising as instrumental variables and found that there was indeed a positive causal effect of sport participation on happiness. @edouard2021 suggested to use instrumental variables for the analysis of sport injury prevention treatments. But @shrier2020 demonstrated that they made several mistakes in both their theoretical presentation and the example data analysis. This does not invalidate the potential use of instrumental variables in sport science, but highlights the caution that has to be taken when implementing new approaches.

The search for good instrumental variables is a challenging one. Aside from the assumptions of the DAG structure in @fig-instr, instruments should be strongly related to the treatment variable. If they only moderately influence the treatment, they are called "weak instruments" [@bound1995]. Weak instruments are often unsuccessful in removing bias, even in large samples. Good instruments often involve an element of randomness. As an example of sport science, we may be interested in the effect of being part of youth national squad on adult sport success. Both are very likely confounded by a variety of unobserved factors (e.g., social, psychological, and biological). An example for an instrument is in this case the month of birth. The month of birth does not share any unobserved confounders with the other variables, as it can be seen as essentially random. But it effects being part of a youth national squad (as the norms are defined by birth years and later born are less matured and thus less likely to be part of the squad). It does however have no direct influence on later adult success. Therefore the DAG in @fig-instr holds, though we have to test if birth month is not a too weak instrument. This example demonstrates the potential use of instrumental variable approaches in sport science.

### Regression Discontinuity

Regression discontinuity is one of the most widespread dedicated methods of causal inferences. First used in psychology by @thistlethwaite1960, it took until the late 1990s to finally gain popularity [@cook2008], to becoming one of the most common designs in causal analysis, particularly in economics [@lee2010]. The basic idea of regression discontinuity is that there is a running variable that decides treatment based on a fixed cut point. Individuals slightly above and below the cut point will likely be similar in terms of observed and unobserved covariates, but they will differ in the treatment they receive. Exceeding the threshold will either always lead to the application of treatment (sharp regression discontinuity design) or disproportionately increase the probability of receiving a treatment (fuzzy regression design) [@imbens2008]. The discontinuity in the outcome at the cut point allows to estimate a (local) causal effect of the treatment.

A key assumption of regression discontinuity is the continuity. It means that we expect no discontinuities in outcome if no treatment had been applied (if the variable exceeding a cut point did not lead to any consequences). Regression discontinuity design are usually analysed by running regressions of the running variable on the outcome for each side of the cut point [@imbens2008]. The difference in expected outcomes at the cut point of the running variable corresponds to the local causal effect of the treatment. To account for non-linear relationships researchers often use polynomials, though this procedure can be misleading [@gelman2021]. Individuals who are aware of the relevance of the threshold may game the treatment assignment by manipulating their running variable value[^discussion-4]. Therefore analysts should check the density and characteristics of individuals near the threshold [@mccrary2008; @barreca2016].

[^discussion-4]: This would bias our causal estimate, as treatment assignment would not be essentially random around the threshold anymore. A famous example is @barreca2011, who showed that an unusual high number of babies slightly fell below a body weight threshold that made them eligible for intense medical care. This suggests that doctors and nurses may have deliberately manipulated the children weight measurement if they believed intense care to be helpful. Not correcting for this bias will result in unreliable results of the regression discontinuity analysis.

As an example from sports take the system of promotion and relegation in sport leagues. The causal effect of promotion and relegation on performance and financial is difficult to estimate, as teams being relegated are likely less strong and financially equipped than teams holding their league. But when only comparing teams that were slightly above or below the cut down for relegation, we can compare teams that probably share a similar background. @speer2023 used this idea to estimate the financial effect of relegation and promotion in sport leagues using a regression discontinuity design.

Other sport application of regression discontinuity come mainly from sport economics. @keefer2016 and @branson2019 investigated the effects of the draft system in American basketball. @hon2016 researched the causes of introducing the three-point rule in the German soccer league. @engist2021 focused on the effect of the seeding system on team performance in soccer tournaments. In an application more related to health care, @fredslund2019 estimated the influence of a holiday break on fitness routines in the general population. In one of the most popular applications of regression discontinuity designs in sport (though published in economic journal), @berger2011 found that being slightly behind in a game at half-time disproportionately increases the chances of winning a basketball game. They speculated that being behind by a slight margin increases motivation and thus causes a performance benefit. But @kleinteeselink2023 showed that the findings by @berger2011 did not hold in a larger sample of more seasons and different sports.

Regression discontinuity is suited for any sport context that involves slight differences in an indicator causing large potentially effects in outcomes. The aforementioned concepts of team relegation, and in general winning a game are thus potential use cases for regression discontinuity analyses in sport. Further applications could be squad nomination based on fixed performance thresholds, tournament seeding based on ranking systems, or the effect on health interventions implemented based on biological measures (e.g., body mass index thresholds). As the field of sports and exercise science is rich of indicator variables that cause treatment by using rather arbitrary break points, a promising potential for using regression discontinuity designs exists.

### Difference-in-difference

Difference-in-difference is the oldest and most common quasi-experimental research design to estimate causal effects from observational data. Its first use can be traced down to John Snow's investigation of the London cholera pandemic in the mid 1850s [@snow1955; @coleman2019]. In the past decades it has evolved into the most popular design in economics and social-science, with one of the most influential application being @card1994 investigating the influence of a minimum wage rise on the labor market. The basic idea of difference-in-difference design is — in the absence of a RCT — to use a natural experiment. Two groups of individuals are observed over a time period, in which one group receives an intervention and the other does not. Both groups may be influenced by unobserved time-varying factors, but if these are constant, the second group can act as the control group for the treatment. Finally, the between-group difference between the two within-group differences is an unbiased estimate of the causal treatment effect.

The key assumption of difference-in-difference designs is the parallel time trend. Without any intervention, we expect both groups to develop in a similar way. To ensure this parallel time trend the choice of the control group is crucial. This is why difference-in-difference is sometimes combined with matching (@sec-balancing) or synthetic control groups (@sec-synth). The treatment and the control group do not necessarily have to be observed over the same time period [@goodman-bacon2021; @callaway2021], and the treatment can also be continuous instead of binary [@callaway2024]. Because difference-in-differences works with time series data, much care has to be taken into calculating appropriate measures of certainty of estimates [@bertrand2004].

Difference-in-difference designs can be found in sport science, but are rarely termed that way and often not systematically analysed from a causal viewpoint. Essentially most analysis of covariance and time\*group interaction analyses can be understood as some form of difference-in-difference. Research in sport science that explicitly use observational data to estimate causal effects using the well-researched difference-in-differences methods are rare, possible because the spread of this design has been mainly limited to economics and social science, but not medical science. Consequently the few articles published stem from the field of sport economics, where we can expect that authors were inspired by research from their mother discipline [@böheim2022; @weimar2022; @budzinski2020]. However the difference-in-difference design has potential for sport science in far more cases when randomized experiments are not feasible, but quasi-experimental research of observational data is possible. This could for example be the effect of rule changes or abrupt technological advancements in sports. <!-- one more sentence? -->

### Synthetic Control {#sec-synth}

Synthetic control is arguably the most novel and thus most developing special causal inference method of the past years [@athey2017]. It was first used by @abadie2003, and first systematically presented in @abadie2010. In synthetic control studies, researchers observe a single time-series of a group-level intervention, for which no adequate control group exists. Therefore a larger set of non-fitting (i.e., differing in covariates) control groups is combines to create a single "synthetic" control group. This combining usually works by weighting covariates of the different groups from the control pool in such a way, that they match the covariates of the treatment group. For the pre-treatment period, the intervention and the synthetic control group should have similar trends in outcomes, and any differences in time trends after the treatment can be causally attributed to the treatment.

Despite its promising and innovative nature, synthetic control methods have not yet been used in sport science. The main reason may be that the research field is quite new even in its home discipline of economics and political science, so that it may take a few more years until first researchers will begin to adopt these methods to sport science. That said, sport science offers many instances, where single-unit time series data could be compares against synthetic control groups. An example is the influence of head coach changes on performance in team sports. Another example is the influence of talent development programs in certain countries on later athlete success.

## Challenges and Limitations

### Need for Theoretical Models

Causal modeling is futile without causal knowledge. While general knowledge of causal modeling is necessary to perform any causal analysis, the specific application to a single research problem requires deep institutional knowledge of the underlying phenomena. Understanding of the causal structure of a research question is something that can barely be provided by a data analyst alone, but that requires expert knowledge of the research area accumulated over years. While the final estimation process may or even should be performed by a trained statistician [@sainani2021], constructing an estimation strategy should be the joint work of people familiar with the statistical methods and people familiar with the specific research field.

Theoretical causal models are always subjective. They are subjective in a way that they rely on personal understanding of a research phenomena, that is not factual per se. But it is a researchers obligation to argue for their understanding of the field and the conclusion they draw for their own work. Essentially, most of science can be seen as subjective, so it should not be the question whether a particular scientific approach is subjective, but whether it is in its inevitable subjectivetly reasonably justified. In addition, scientific knowledge is not written in the stones, but a constantly evolving construct, that heavily relies on its social component and is ultimately determined not by universal truth but by consensus. Therefore the subjectivity of causal models that build the foundation of causal inference is not a limitation, but an inherent feature of most scientific research.

Non-causal inference also relies on theoretical models, but is often less transparent in communicating them. Essentially all data analyses embed some form of institutional, as well as statistical assumptions. Researchers without appropriate background often do not realize these assumptions, as they are often made implicitly and rarely presented. Causal inference, by constrast, explicitly demands a discussion of these assumptions during the process of developing a causal model. This makes it easier to discuss and understand the implications of assumptions for the research. The need for theoretical models, which may seem like a needless extra effort in the first place, turns out to be more of a strength than a limitation of causal inference.

### Complex Systems

When I first planed the structure of this thesis I only had a fairly limited understanding of causal inference as a research field. One of the early ideas I had in mind (see the preregistration) was to showcase the process of developing a causal model in a research field I consider myself fairly experienced it (endurance running). However during the writing of this thesis I struggled with the idea and finally abandoned my initial plan: My naive self had made two mistakes in the beginning: First it is extremely difficult to draw a general causal model of a research field, and causal models are rather specific to the exact research question. Second it is nearly impossible to develop any exhaustive model, and causal models are always simplifications from reality.

Any causal model may depend on the exact research context and research question. Running performance in the laboratory may have different underlying causal influence than running performance in a simulated field-performance, which causal model may again differ from that of an actual performance in a race. Without any specific causal research question that clearly defines the estimate of interest in its context, the causal model may vary. Thus it is rather impossible to define a general causal model that, say allows to answer all questions regarding the causes of endurance running performance. While the first papers introducing graphical causal models in sport science use such general models for injury research [@shrier2007] and strength training responses [@steele2020], these models rather function as toy examples to illustrate how causal modeling may work rather than as blueprint for future studies using causal inference in these fields. The causal models required for causal inference do not only rely on the structure of the underlying research problem, but also on the exact research question of interest.

Most systems analysed in sport embedded an extreme complexity — ultimately we are investigating human behavior that is determined by a set of biological, psychological, social, and environmental factors. It may be a tempting yet impossible task to include all potential causes of sport performance in the structure of a causal model. Therefore any model is wrong in a sense that it will never capture the full reality. But this should not be the aim of any model (whether causal or not) in first place. Models always make simplifications of the reality, and these simplification often bring their own benefits in that they allow to focus on the question of interest. Causal inference does not need exhaustive DAGs, but sufficient DAGs that capture those causal relationships that may be interesting in terms of bias for the causal estimand. Even if a researcher ascertains that the current knowledge is not good enough to draw a sufficient DAG for a research question, other causal information or incomplete DAGs can provide valuable information for causal research. Essentially, a lot of the identification methods discussed in the previous @sec-ident work without a complete understanding of the causal model of the researched topic. For example, the question which variables to control for in a statistical analyses to reduce bias in estimating causal effects can be answered fairly well in the absence of a full DAG (e.g., the question of which variables to condition on, see @sec-control). It will remain impossible to completely understand the complex systems present in sport science, but causal inference provide a systematic approach to simplify these systems in causal models, which ultimately allows to find causal relationships in these systems.

### Small Samples

Causal inference cannot overcome difficulties inherent to small sample sizes. Small sample sizes generally yield imprecise estimates, or if using a hypothesis testing framework, they have low statistical power. One of the main goals of causal inference is to provide unbiased estimates, but it only scratches the surface of the problem of precision. In other words, in a small sample even an unbiased estimate may strongly vary based on sampling variation and thus be useless in practice. However in larger observational samples the uncertainty created by bias is generally much larger than that by sampling, demonstrating the necessity of causal inference methods. Some of the identification methods discussed in @sec-ident require rather large data sets (e.g., instrumental variables or some matching procedures), while others can in theory work on a relatively small sample (e.g., difference-in-difference, synthetic control). The problem of small samples is something that causal inference can neither solve, nor is this a designated goal of it. To deal with small sample sizes in sport other approaches have to be taken, such as reconsidering study design and data analysis choices (e.g., including outside-the-trial knowledge in the data analysis with Bayesian methods) [@hecksteden2022]. Small sample sizes that are common in sport science limit the utility of causal inference, as they do for any other statistical method.

### Data Quality

<!-- Even a correct model can not answer the questions if it has the wrong data. Incorrect or noisy data limits research, and causal inference can not really help here. Sometimes no perfect model exists. -->

<!-- causal inference can not compensate bad experimental design -->

<!-- however this is an issue for all types of analysis. Causal inference has tools to deal with error and missingness of data -->

## Perspectives and Further Possibilities

### Modeling Missing Data and Measurement Error

<!-- maybe measurement error in VO2max or RPE as proxy of motivation -->

<!-- link to imputation -->

<!-- sampling bias? -->

### Heterogenous Effects

<!-- something about effect homogenity/heterogenity, maybe sharp nulls, and maybe individual-level predictions (e.g., counterfactual calculation by Pearl and their limitations) -->

### Longitudinal Data

<!-- DAGs are fixed in time. Can be used for longitudinal data if time-points are separate notes. Make example of exercise intervention. Maybe SEM /Robins DAG as an alternative -->

### Understanding Big Data

<!-- Large amount data. Currently analyzed with ML and traditional LR. May be good for prediction, but often does not help understanding. Even for prediction tasks understanding is vital. Understand why a model is failing in some cases, how a system works in new environments, and how to actually interfere. -->

<!-- Maybe (Hypothetical) example of predicting marathon performance with training data -->

### Communicating Causality

<!-- something about causal and non-causal language, stating assumptions, etc -->

<!-- maybe sometimes to confident usage of causal inference -->

<!-- but more explicit with assumptions than current practices -->

## Causal Modeling Workflows in Sport Science Practice

<!-- Implementation of causal modeling is part of a general revision of the scientific workflow in sport science. Transparent documentation of all steps (links to preregistration, reproducibility) -->

<!-- combining expert domain knowledge (expertise) for creating models and outside domain knowledge (data analysis skills/statistics) -->

<!-- First: Identify task as description, prediction, causal inference -->

<!-- Second: State your estimand (and create a theoretical model, implement prior beliefs) -->

<!-- Third: Plan study or data analysis accordingly-->

<!-- Fourth: Model the data accordingly -->

<!-- Test assumptions, robustness, sensitivity -->

<!-- Communicate process, results and assumptions -->
