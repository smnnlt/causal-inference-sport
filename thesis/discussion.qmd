---
title: ''
format: pdf
bibliography: ../references.bib
editor:
  markdown:
    references: 
      location: block
      prefix: "discussion"
---

```{r}
#| label: setup2
#| echo: false
library(ggdag)
library(dagitty)
```

# Discussion

The aim of this thesis was to introduce methods of causal inference to sport science. After an introduction to graphical causal models, I demonstrated the utility of adopting a causal viewpoint in planning research and analyzing observational data through two examples. First, I will discuss the general application of causal inference in sport science and present special causal inference methods and their potential use in sport science. Then, I will address the limitations of causal modeling to ultimately derive a workflow for implementing causal modeling into the sport research process.

## General Applications of Causal Inference in Sport Science

Causal questions in sport science require causal answers. Because of the infeasibility of randomized controlled trials in many instances [@bullock2023], much of the research relies on observational data [@abt2022]. However, causal inference structures and tools to aid these analyses are largely absent from the sport science literature [@kalkhoven2024]. This can lead to unreliable and overly speculative findings [@kalkhoven2024]. Without these support tools, causal analyses of observational data in sport science are prone to bias, as shown in real examples [e.g., @smoliga2017]. Implementing causal inference methods in sports research can help identify true causal effects [@stovitz2019; @shrier2007; @nielsen2020].

The current absence of causal inference methodology in sport science can be possibly attributed to a lack of knowledge in the field. Research articles dealing with causal inference in sport science are rare and are mainly limited to a handful of editorials. While these few articles have been published in high-profile sport science journals [e.g., @stovitz2019], they are often authored by individuals from outside sport science (e.g., epidemiology) and the sport science community is far from adopting the suggested methods.

Correctly applying causal inference techniques requires both a deep understanding of the domain of interest and a solid grasp of fundamentals of causal inference. As demonstrated in this thesis, causal inference is a complex field with various (and sometimes competing) frameworks and a variety of methods to consider. Given that deficits in basic statistical knowledge sometimes appear in published sports research [@sainani2021], it seems excessive to demand the teaching of causal inference in sport science curricula. However, a fundamental understanding of key concepts of causal inference (e.g., the idea of potential outcomes, confounder bias, and colliders) appear to be not just helpful but necessary for conducting any research addressing causal questions [@kalkhoven2024]. These methods can be taught even at the undergraduate level and in field-specific educational articles, as seminal texts from psychology [@rohrer2018] and epidemiology [@hern√°n2004] demonstrate. Such texts are currently missing from the sport science literature, as existing articles focus on the subfield of injury research [e.g., @kalkhoven2024; @shrier2007] or barely scratch the surface of causal inference [@nielsen2020b; @nielsen2020]. The most compelling text so far is probably by @stovitz2019, but its editorial format limits the amount of content presented. An accessible introduction to causal inference for sport science has yet to be written and published.

As with any discussion on research methods, the application of causal inference in sport science must be viewed within the broader picture of scientific practice. Using causal inference does not eliminate other problems in research (see @sec-limits), nor is it a (partial) solution [@briggs2023] to the replication crisis currently affecting science[^discussion-1]. Instead, its adoption should complement other measures to increase research quality in sport science, such as data sharing and preregistration [@caldwell2020]. Individual sport researcher who do not adopt causal inference methods for addressing causal questions cannot be blamed, as long as external incentives for doing so are lacking. For example, collaborating with statistical experts seems ideal [@sainani2021], but is often impossible for small to medium research projects with limited resources. Learning appropriate causal inference methods by oneself requires large amounts of time, which researchers might prefer to allocate elsewhere, especially if causal inference is not yet standard in sport science publications. However, introducing causal inference in sport science by those intrinsically motivated to improve their research quality may lead to more early adopters and ultimately change the norms of the scientific field.

[^discussion-1]: Regarding replication issues, sport science is more of an illustrative example than an exception [@mesquida2022], as primarily results from @murphy2024 demonstrate.

## Applicability of Special Causal Inference Methods in Sports {#sec-ident}

Apart from general principles of causal thinking and modeling based on graphical representations, a set of special causal inference methods has gained popularity in the past decades. Based on the potential outcome framework [@rubin1974], these methods have become standard tools in the analysis of observational data, especially in the field of economics [@athey2017], but also beyond. @angrist1999 called these tools "identification methods", because they help to identify causal effect estimates in certain common situations. These well-researched analysis tools may also prove helpful in many applications in sport science. I will here introduce five common identification methods and discuss their potential application in sport science[^discussion-2]. @tbl-ident provides a summary of these five methods.

[^discussion-2]: I chose the set of five methods based on @angrist1999 while making some modifications. I replaced the general "conditioning in a regression model" strategy, which has been discussed earlier, with covariate balancing methods, and added the newer method of synthetic control [both to some extent inspired by @athey2017; @cunningham2021].

<!-- overview in a table (landscape mode) -->

```{=tex}
\newpage
\KOMAoptions{paper=landscape,pagesize}
\recalctypearea
```
```{r}
#| echo: false
#| label: tbl-ident
#| tbl-cap: "A summary of causal identification methods and their application to sport science."
#| message: false

d <- data.frame(
  names = c("Covariate Balancing", "Instrumental Variables", "Regression Discontinuity", "Difference-in-Difference", "Synthetic Control"),
  idea = c("Creating groups balanced on observed covariates when group assignment was not random","Control for unobserved confounders by using a variable that only relates to the outcome via the treatment.", "Finding a treatment that is assigned based on a certain threshold, to compare individuals slightly above and below this threshold","Observing a quasi-experimental treatment and control group over time to estimate treatment effects", "Comparing a single time-series  to a synthetic control time-series based on imperfect control groups"),
  source = c("@stuart2010", "@greenland2000", "@imbens2008", "@lechner2011", "@abadie2021"),
  appl = c("Genetic profiling, injury research, team sport analytics", "Non-compliance and measurement errors in sport interventions, talent development", "Effects of winning and relegation, draft systems, squad nominations", "Rule changes, technological developments", "Coach changes, talent development programs")
)

flextable::qflextable(d) |>
  flextable::set_header_labels(values = c("Method", "Basic Idea", "Reference", "Applications in Sport Science")) |>
  flextable::width(width = c(2, 3, 1.5, 1.5)) |>
  ftExtra::colformat_md()

```

```{=tex}
\newpage
\KOMAoptions{paper=portrait,pagesize}
\recalctypearea
```
### Covariate Balancing {#sec-balancing}

A common approach to causal inference from observational data is to mimic the characteristics of a RCT. In an RCT, treatment assignment is random, and thus treatment groups only differ in their covariates by chance. Conversely, in observational studies, covariates may influence treatment assignment. For example in the previously examined study of multiple running shoe use and injury risk [@malisoux2015, @sec-example1], runners may decide if they use different shoes based on their training characteristics. If training characteristic also influence the outcome parameter of injury risk, this is an classical example of confounder bias (see @fig-confcoll a). To mimic a RCT of multiple running shoe use, we could decide to only compare individuals with similar training characteristic (and other covariates). This is the basic idea of covariate balancing.

Covariate balancing can broadly be classified into three categories: subclassification, matching, and reweighting [@stuart2010]. Subclassification involves grouping individuals with similar covariates into subclasses, comparing different treatments only within these subclasses, and then calculating a weighted average of these comparisons [@cochran1968]. Matching aims to find individuals with equal or similar covariates and compare only them, often on a 1:1 basis, before pooling all comparisons [@rubin1973]. This often involves discarding data for which no (sufficient) matches could be found. Reweighting keeps all observation, but assigns them new weights based on how representative they are for their group.

Regardless of the method used, covariate balancing is typically a pre-analysis routine, meaning it occurs before the actual causal data analysis and does not include information on the outcome values. In some sense, it aims to address the same problem of observed confounders as simple conditioning in a regression does address. Current advice suggests using covariate balancing not as a replacement for regression adjustments but as a complementary approach, such as in the so called "doubly-robust" methods [@bang2005]. A benefit of covariate balancing over regression adjustments is that it facilitates the checking of overlap in covariate distributions. If this overlap is not present, linear adjustments may perform poorly as they must rely on extrapolation, a factor often not checked in standard linear regression workflows. Ultimately, whether to use regression adjustment or covariate balancing methods to estimate a causal effect depends on the individual's assessment of the contextual advantages and drawbacks of each strategy[^discussion-3].

[^discussion-3]: We live in fascinating times for research. Answer posts on the online discussion forum Stack Exchange can be more detailed, informative, and engaging than possibly any journal article could ever be, as it is the case for Noah Greifer's answer on whether to prefer regression-based methods or matching for causal inference [@greifer2021].

A critical question in covariate balancing is defining "closeness" or similarity of covariates. Exact equality on all covariates is feasible only for large samples with few discrete covariates. In most cases, researchers need to compute distance measures. One of the most common measures is the propensity score, the conditional probability that an individual was assigned to the treatment group given one's covariates [@rosenbaum1983; @dehejia2002]. The propensity score reduces the multidimensional covariates to a single value, which can be used to form subclasses, match units, or weight observations. The exact choice of balancing method and distance measure should be context-specific, and the scientific debate about which procedure works best is vivid[^discussion-4]. @stuart2010 offers general recommendations regarding the procedure selection. Usually, it is beneficial to compare different methods (and their parameters) within the given data set.

[^discussion-4]: To provide a short overview of the debate: @fr√∂lich2004 argues that weighting is always inferior to matching, a claim contested by @busso2014. @iacus2011 heavily criticize the widely used propensity-score matching [@dehejia2002], proposing instead their own method of coarsened exact matching [@iacus2011; @iacus2012], which has in turn faced opposition from @black2020.

The first researchers have begun to adopt covariate balancing methods in sport science. In the field of injury rehabilitation, propensity score matching was used to find suitable control groups for athletes undergoing recovery [@farinelli2023; @owens2022; @fenn2023; @jimenez2022]. Other studies compared fitness levels in soccer players across different decades [@gonaus2023] and estimated the effect of playing venue on match outcomes [@kneafsey2018] using various matching methods. @nakahara2023 used propensity-score based subclassification to evaluate pitching strategies in baseball. In an innovative study, @gibbs2022 applied matching to investigate the effect of timeouts on stopping point runs in basketball games.

In general, covariate balancing can be beneficial in sport science scenarios where there is a limited numver of interventions, a set of observed pre-treatment variable, and relatively large sample sizes. While covariate balancing methods can help reduce bias, matching effectively discards observation units and is typically unstable with small sample sizes. Therefore, these methods are particularly suited for contexts with a sufficiently large control group. For example, this could involve comparing physiological markers between injured and uninjured athletes or analyzing genetic profiles of elite athletes versus a non-elite population. Covariate balancing can also be applied in non-randomized studies investigating the health benefits of recreational sport participation. Additionally, it can aid in understanding the causal mechanisms behind team sport performance [e.g., @gibbs2022].

### Instrumental Variables

Confounders between the received treatment and outcome are often unobserved or even unknown, making it challenging to estimate an unbiased causal treatment effect directly.¬†One approach to reduce bias is to use an instrumental variable. Instrumental variables are those that influence the outcome only mediated by the treatment. For a graphical representation see @fig-instr. When a set of unobserved variables $U$ affects both the treatment $X$ and the outcome $Y$, their causal relationship is biased because we cannot control for the unobserved $U$. If we instead use the instrumental variable $Z$, which affects $X$ directly and $Y$ only indirectly via $X$, we can isolate the part of the effect of $X$ on $Y$ that is not influenced by $U$.

```{r}
#| label: fig-instr
#| out-width: '50%'
#| fig-scap: "A graphical example of an instrumental variable."
#| fig-cap: "A graphical example of an instrumental variable. The relationship between exposure $X$ and outcome $Y$ is confounded by a set of unobserved variables $U$. The instrumental variable $Z$, which is unconfounded and affects $Y$ only via $X$, can be used to provide an unbiased estimate of the causal effect of $X$ on $Y$. "

dag_instr <- dagify(X ~ Z + U, Y ~ X + U)
coordinates(dag_instr) <- list(x = c(Z = 0, X = 0.5, U = 0.75, Y = 1), y = c(Z = 0, X = 0, U = 0.2, Y = 0))

p_instr <- ggdag_classic(dag_instr, size = 15) + theme_dag_blank()
p_instr
```

A classic example of an instrument is random treatment assignment [@greenland2000]. People may actively decide whether to receive an assigned treatment, and these decisions could be influenced by unobserved confounders that also affect the outcome variable. While the assignment to a treatment does not directly influence the outcome, the actual receipt of the treatment does. Because individuals assigned to the treatment group are much more likely to follow the assignment and thus receive the treatment, the treatment assignment can serve as an instrument. By using an appropriate procedure (typically a two-stage least square estimator), researchers can estimate the causal effect of receiving a treatment on the outcome[^discussion-5].

[^discussion-5]: Depending on whether we assume heterogeneity in the treatment effect, the causal effect estimated by using an instrumental variable is somewhat limited in its definition. Strictly speaking, we are estimating the causal effect of treatment only in those who adhere to the treatment assignment. This is known as the locale average treatment effect for compliers. If the focus is on the mechanistic consequences of receiving a treatment, this estimate can be quite relevant. However, if the goal is to evaluate the impact of implementing a treatment at a population level (e.g., for policy research), it is more appropriate to account for non-compliance in the estimation. In such cases, using instrumental variables may not be suitable.

Instrumental variables can be used to adjust for unobserved confounders and are particularly useful in addressing issues of measurement error and non-compliance. For instance, in sport science, @ruseski2014 investigated the causal effect of sport participation on happiness, a relationship that is likely confounded by unobserved variables such as biography and socio-economic background. They used physical distance to the nearest sporting facility and personal belief in the benefits of exercising as instrumental variables and found a positive causal effect of sport participation on happiness. @edouard2021 suggested using instrumental variables to analyze sport injury prevention programs. But @shrier2020 demonstrated that they made several severe mistakes in both their theoretical presentation and example data analysis. Their critique does not negate the potential value of instrumental variables in sport science but underscores the need for caution when adopting new methodological approaches.

The search for good instrumental variables is challenging. Aside from adhering to the assumptions depicted in the DAG structure in @fig-instr, instruments should be strongly related to the treatment variable. Instruments that only moderately influence the treatment, are termed "weak instruments" [@bound1995]. Weak instruments often fail to remove bias, even in large samples. Good instruments frequently involve an element of randomness. As an example in sport science, we might want to investigate the effect of being part of a youth national squad on adult sport success. Both variables are likely confounded by various unobserved factors, such as, social, psychological, and biological influences. An potential instrument in this context could be the month of birth. The month of birth does not share any unobserved confounders with the other variables, as it can be seen as essentially random. But it affects being part of a youth national squad, because squad selection norms are based on birth years, with later-born being less mature and thus less likely to be selected. The month of birth, however, does not directly affect later adult success. Hence, the DAG in @fig-instr remains applicable, though we have to verify that the month of birth is not a weak instrument. This example highlights the potential of using instrumental variable approaches in sport science.

### Regression Discontinuity

Regression discontinuity is one of the most widespread special methods of causal inferences. First introduced in psychology by @thistlethwaite1960, it gained popularity in the late 1990s [@cook2008] and has become one of the most common designs in causal analysis, particularly in economics [@lee2010]. The basic idea of regression discontinuity is that there is a running variable determining treatment based on a fixed cut point. Individuals slightly above and below the cut point are likely to be similar in terms of observed and unobserved covariates but will differ in the treatment they receive. Exceeding the threshold will either always lead to the application of treatment (sharp regression discontinuity design) or disproportionately increase the probability of receiving a treatment (fuzzy regression design) [@imbens2008]. The discontinuity in the outcome at the cut point allows for the estimation of a (local) causal effect of the treatment.

A key assumption of regression discontinuity is continuity. It means that if no treatment were applied (i.e., if the variable exceeding a cut point did not lead to any consequences), we would expect no discontinuities in the outcome variable. Regression discontinuity designs are typically analyzed by running regressions of the running variable on the outcome for each side of the cut point [@imbens2008]. The difference in expected outcomes at the cut point of the running variable corresponds to the local causal effect of the treatment. To account for non-linear relationships, researchers often use polynomials in their models, although this approach can be misleading [@gelman2021]. Additionally, people aware of the relevance of the threshold may manipulate their running variable values to game the treatment assignment[^discussion-6]. Therefore, analysts should check the distribution density and covariate characteristics of individuals near the threshold to detect such manipulation [@mccrary2008; @barreca2016].

[^discussion-6]: This manipulation would bias our causal estimate, as treatment assignment would no longer be essentially random around the threshold. A famous example is provided by @barreca2011, who showed an unusual high number of babies slightly just below the body weight threshold that made them eligible for intense medical care. This suggests that doctors and nurses may have deliberately manipulated weight measurement if they believed intense care to be beneficial. Not correcting for this bias yields unreliable results of the regression discontinuity analysis.

As an example from sports, consider the system of promotion and relegation in sport leagues. Estimating the causal effect of promotion and relegation on performance and financial outcomes is difficult, as teams being relegated are generally weaker and less financially equipped than teams remaining in their league. However, by only comparing teams that were just slightly above or below the cutoff for relegation, we can compare teams with likely similar background. @speer2023 used this approach to estimate the financial effect of relegation and promotion in sport leagues using a regression discontinuity design.

Other application of regression discontinuity in sports primarily come from sport economics. For instance, @keefer2016 and @branson2019 investigated the effects of the draft system in American basketball. @hon2016 researched the impact of introducing the three-point rule in the German soccer league. @engist2021 focused on the effect of the seeding system on team performance in soccer tournaments. In a health-related application, @fredslund2019 estimated the influence of a holiday break on fitness routines in the general population. One of the most popular applications of regression discontinuity designs in sport, although published in an economic journal, was by @berger2011. They found that being slightly behind at half-time disproportionately increases the chances of winning a basketball game. They speculated that being behind by a slight margin increases motivation, leading to a performance benefit. But @kleinteeselink2023 showed that the findings by @berger2011 did not hold in a larger sample covering more seasons and different sports.

Regression discontinuity is suited for any sport context where slight differences in an indicator cause large potentially effects in outcomes. The aforementioned concepts of team relegation and winning a game are prime use cases for regression discontinuity analyses in sport. Additional applications could include squad nomination based on fixed performance thresholds, tournament seeding based on ranking systems, or the effect of health interventions implemented based on biological measures (e.g., body mass index thresholds). Since the field of sports and exercise science is rich with indicator variables that cause treatment using somewhat arbitrary break points, a considerable potential for using regression discontinuity designs exists.

### Difference-in-difference

Difference-in-difference is the oldest and most common quasi-experimental research design to estimate causal effects from observational data. Its first use can be traced down to John Snow's investigation of the London cholera pandemic in the mid-1850s [@snow1855; @coleman2019]. Over the past decades, difference-in-difference has evolved into the most popular design in economics and social science, with one of the most influential application being @card1994 investigating the influence of a minimum wage rise on the labor market. The basic idea of difference-in-difference design is to use a natural experiment in the absence of a RCT. Two groups of individuals are observed over a period of time, in which one group receives an intervention and the other does not. Both groups may be influenced by unobserved time-varying factors, but if these factors are constant, the second group can act as the control group for the treatment. The between-group difference between the two within-group differences provides an unbiased estimate of the causal treatment effect.

The key assumption of difference-in-difference designs is the parallel time trend. Without any intervention, we expect both groups to develop similarly. To ensure this parallel time trend, the choice of the control group is crucial. This is why difference-in-difference is sometimes combined with matching (@sec-balancing) or synthetic control groups (@sec-synth). The treatment and the control group do not necessarily have to be observed over the same time period [@goodman-bacon2021; @callaway2021], and the treatment can also be continuous instead of binary [@callaway2024]. Because difference-in-differences works with time series data, great care must be taken to calculate appropriate measures of uncertainty for estimates [@bertrand2004].

Difference-in-difference designs can be found in sport science, but they are rarely termed as such and often not systematically analysed from a causal perspective. Essentially, most analysis of covariance and time\*group interaction analyses can be understood as some forms of difference-in-difference. Research in sport science that explicitly uses observational data to estimate causal effects with well-researched difference-in-differences methods is rare, likely because the spread of this design has been mainly limited to economics and social science rather than medical science. Consequently, the few articles published stem from sport economics, where authors are likely inspired by research from their primary discipline [@b√∂heim2022; @weimar2022; @budzinski2020]. However the difference-in-difference design has potential for sport science in many situations where randomized experiments are not feasible, but quasi-experimental research of observational data is possible. This includes examining the the effect of rule changes or abrupt technological advancements in sports. For example, the effects of video assistant referees in soccer or new shoe developments in running could be studied using difference-in-difference methods.

### Synthetic Control {#sec-synth}

Synthetic control is arguably the most novel and rapidly developing special causal inference method in recent years [@athey2017]. It was first introduced by @abadie2003 and first systematically presented by @abadie2010. In synthetic control studies, researchers observe a single time-series of a group-level intervention, for which no adequate control group exists. Therefore, a larger set of non-fitting (i.e., differing in covariates) control groups is combines to create a single "synthetic" control group. This is typically done by weighting the covariates of the different groups from the control pool to match the covariates of the treatment group as closely as possible. For the pre-treatment period, the intervention and the synthetic control group should display similar trends in outcomes. Any differences in time trends after the treatment can then be causally attributed to the treatment.

Despite its promising and innovative nature, synthetic control methods have yet to be adopted in sport science. The primary reason may be that this research technique is still relatively new, even within its original fields of economics and political science. It may take several more years before researchers in sport science begin to apply these methods. Nevertheless, sport science presents numerous opportunities for employing synthetic control analyses. For instance, one could investigate the effect of head coach changes on team performance. Another example is evaluating the effectiveness of talent development programs in certain countries on the later success of athletes.

## Challenges and Limitations {#sec-limits}

### Need for Theoretical Models

Causal modeling is futile without causal knowledge. While general knowledge of causal modeling is necessary to perform any causal analysis, applying it to a specific research problem requires deep institutional knowledge of the underlying phenomena. Understanding of the causal structure of a research question is something that can hardly be provided by a data analyst alone; it requires expert knowledge of the research area accumulated over years. While the final estimation process may or even should be performed by a trained statistician [@sainani2021], constructing an estimation strategy should be the joint work of individuals familiar with statistical methods and those with expertise in the specific research field.

Theoretical causal models are inherently subjective. They rely on personal understanding of a research phenomena, which is not factual per se. But it is a researcher's obligation to justify their understanding of the field and the conclusion they draw for their work. Essentially, most of science can be seen as subjective, so the question should not be whether a particular scientific approach is subjective, but whether it is reasonably justified in its inevitable subjectively. In addition, scientific knowledge is not set in stone; it is a constantly evolving construct that heavily relies on its social component and is ultimately determined by consensus rather than universal truth. Therefore, the subjectivity of causal models, which form the foundation of causal inference, is not a limitation but an inherent feature of most scientific research.

Non-causal inference also relies on theoretical models, but these models are often less transparently communicated. Essentially all data analyses embed some form of institutional and statistical assumptions. Researchers without appropriate background may not realize these assumptions, as they are often made implicitly and rarely presented. Causal inference, by contrast, explicitly demands a discussion of these assumptions during the process of developing a causal model. This transparency makes it easier to discuss and understand the implications of assumptions for the research. The need for theoretical models, which may initially seem like a needless extra effort, ultimately proves to be more of a strength than a limitation of causal inference.

### Complex Systems {#sec-complex}

When I first planned the structure of this thesis, my understanding of causal inference as a research field was fairly limited. One of my early ideas (as written in the preregistration) was to showcase the process of developing a causal model in a research field I consider myself experienced in (endurance running). However, during the writing process, I struggled with this idea and eventually abandoned my initial plan. My naive self had made two mistakes from beginning: First, it is extremely difficult to draw a general causal model of a research field, and causal models are rather specific to the exact research question. Second, it is nearly impossible to develop any exhaustive model, as causal models are always simplifications of reality.

Any causal model depends heavily on the exact research context and question at hand. For instance, the causal determinants of running performance in the laboratory might differ from those in a simulated field-performance, and both might again differ from those in an actual race. Without a specific causal research question that clearly defines the estimate of interest within its context, the causal model can vary. Thus it is nearly impossible to define a general causal model that could answer all questions regarding the causes of endurance running performance. While the first articles introducing graphical causal models in sport science present such general models for injury research [@shrier2007] and strength training responses [@steele2020], these models function more as illustrative examples of how causal modeling might work rather than as blueprints for future studies using causal inference in these fields. The causal models required for causal inference do not only rely on the structure of the underlying research problem but are also influenced by the exact research question of interest.

Most systems analyzed in sport embody an extreme complexity ‚Äì ultimately we are investigating human behavior, which is determined by a set of biological, psychological, social, and environmental factors. It may be a tempting yet impossible task to include all potential causes of sport performance in the structure of a causal model. Therefore, any model is "wrong" in the sense that it will never capture the full reality. But this should not be the primary aim of any model (whether causal or not). Models always simplify reality, and these simplification often bring their own benefits by allowing a focus on the question of interest.

Causal inference does not require exhaustive DAGs, but rather sufficient DAGs that capture those causal relationships relevant to bias for the causal estimand. Even if a researcher determines that the current knowledge is not sufficient to draw a complete DAG for a research question, other causal information or incomplete DAGs can still provide valuable insights. Essentially, many of the causal inference methods discussed in @sec-ident work without a complete understanding of the causal model of the researched topic. For example, determining which variables to control for in a statistical analyses to reduce bias in estimating causal effects can be answered fairly well without a full DAG [@vanderweele2011; @vanderweele2019]. It will remain impossible to completely understand the complex systems present in sport science. However, causal inference provides a methodical approach to simplify these systems in causal models, ultimately enabling researcher to find causal relationships within these systems.

### Small Samples

Causal inference cannot overcome difficulties inherent to small sample sizes. Given that a large portion of the sports science literature uses small samples [@abt2020], this is particularly relevant for the field. Small sample sizes generally yield imprecise estimates, or, if using a hypothesis testing framework, they have low statistical power. One of the main goals of causal inference is to provide unbiased estimates, but this only scratches the surface of the problem of precision. In other words, even an unbiased estimate may vary greatly in a small sample based on sampling variation, rendering it practically useless. However in larger observational samples, the uncertainty created by bias is generally much larger than that caused by sampling variation, highlighting the necessity of causal inference methods. Some of the causal inference methods discussed in @sec-ident require rather large data sets (e.g., instrumental variables or some matching procedures), while others theoretically work on relatively small samples (e.g., difference-in-difference, synthetic control). The problem of small samples is something that causal inference can neither solve, nor is this a designated goal of it. To deal with small sample sizes in sport, other approaches must be taken. This could include reconsidering study design and data analysis choices (e.g., including outside-the-trial knowledge in the data analysis with Bayesian methods) [@hecksteden2022]. Small sample sizes, which are common in sport science, limit the utility of causal inference just as they do for any other statistical method.

### Data Quality

Even an adequate model cannot answer causal questions if it has the wrong data. Data quality is a critical issue, strongly related to small sample sizes, as both decrease the precision of estimates. Sometimes, issues of data quality may be much more important than the often-discussed issues of sample size [@meng2018]. In sport science, many commonly measured variables (e.g., physiological markers such as the maximum oxygen uptake or biological markers of training effects such as genome transcription activity) are noisy because of both biological and technical variation. In exercise interventions studies, participants may drop out for various reasons. Missing data and measurement error are two key points that hinder a causal interpretation of research findings.

Causal inference cannot solve the problem of data quality, but it offers tools to explicitly deal with measurement issues. While many popular tools to address measurement uncertainty are not uniquely causal [e.g., multiple imputation for missing data analysis, @schafer1999], others stem from causal inference [e.g., instrumental variables for measurement errors, @hu2008], or can be directly embedded into a causal framework [@edwards2015; @vansmeden2021].¬†Moreover, considering a causal model during the study planing process can help increase data quality by discussing upfront what variables to measure and how to measure them. Causal inference cannot compensate bad experimental design and low data quality. But it helps to handle these issues during research design and analysis.

### This Thesis

Causal inference often involves scrutinizing research and analysis methods, a procedure that should also apply to this thesis. First, the application of this thesis is limited to empirical research. Essentially, I propose a research framework (see @sec-workflow) that does not apply to my research in this thesis. This serves as a reminder that, even if the contents of this thesis are often aimed to be broadly applicable, they are limited in their actual scope.

Second, staying on a conceptual level may lead to overstating the practical usability of the proposed methods. While I revisited two example studies, I did not reanalyze them using the original data. Therefore, the proposed causal inference methods in @sec-results may not work when analyzing the actual data. Clearly, conducting research is always more complicating than theorizing about research. While an actual causal reanalysis of sport science studies would probably underscore the point I am trying to make with this work, it would be far outside the scope of the thesis and would probably constitute a thesis on its own. Instead, throughout this thesis, I point to the few published examples from sport science, as well as to classic examples from other fields, where causal inference has proven helpful in conducting research.

Third, aside from the numeric biases that causal inference aims to reduce, other forms of bias can influence science, including this thesis.¬†Over the past years, my perspective as a researcher has been shaped by reading scientific articles, blogs, and social media posts related to statistics, methods, and meta-science content. This has influenced my personal view on what science does looks like and how it should look like. My often critical stance towards aspects of current sport science literature and my belief that some sort of methodological reform is necessary were present before I began working on this thesis. Therefore, I may have been biased towards the question whether causal inference is beneficial for sport science, although I believe to present convincing arguments for it throughout this thesis. Others may perceive that sport science has progressed well without causal inference in the past decades, and that methodological reforms impose an unnecessary burden on researchers. But I argue that despite apparent continouos scientific process, the stagnation in many key areas of sport science indicates the need of change [@lohmander2015; @impellizzeri2021; @halperin2018]. Additionally, the scientific method of systematic investigation should not be limited to phenomena but should also apply to science itself. A general cautious openness to new methodological approaches should, therefore, be present in sport science, though their usefulness must ultimately be determined in practice. As stated in several instances throughout this thesis, regardless of the method used, transparency and awareness of one's own biases are crucial elements in the progression of science.

Finally, this thesis is not exhaustive. Causal inference is a vast field in its own with numerous text books written about it, making it challenging to condense into a masters thesis. The existence of different causal inference frameworks and a variety of methods further complicates navigating the scientific landscape, particularly in sport science, where previous work in this area is limited. I chose to combine elements from the two most popular frameworks of causal inference, which may draw criticism from proponents of each. However, I believe that both frameworks offer valuable insights for researchers new to causal inference [@morgan2014; e.g., @didelez2022; @rohrer2024]. In this thesis, I did not address additional sub-fields of causal inference, such as causal discovery [@glymour2019], causal machine learning [@peters2017], or complex causal data structures [@ogburn2024]. I do believe these sub-fields are crucial for certain research problems in sport science, but they are too specific for the general introduction to causal inference provided here. Ultimately, this thesis should be viewed as a starting point for further research into the application of causal inference methods in sport science.

## Causal Modeling Workflows in Sport Science Practice {#sec-workflow}

Data analysis is more than just running a single model; it constitutes a whole workflow of choosing, running, evaluating, and interpreting models [e.g., @gelman2020]. Ideally, the statistical workflow should be tightly integrated with the scientific workflow. Throughout this thesis, we have seen that causal inference plays a role at different stages of the scientific workflow. To place the causal inference methods discussed here into a broader context, I propose a research workflow for sport science that implements causal modeling practices. This workflow aims to provide sport scientists with an overview on how to integrate principles of causal inference into their research processes. @tbl-workflow provides an overview of the different steps, which I will explain in detail in the following sections.

```{r}
#| echo: false
#| label: tbl-workflow
#| tbl-cap: "A research workflow for integrating elements of causal inference into sport science."
#| message: false

d2 <- data.frame(
  stage = c("1. State a research question and a theoretical estimand", "2. Identify your empirical estimand", "3. Estimate the estimand", "4. Perform robustness checks", "5. Interpret the results"),
  datails = c("State a precise research question, specify the type of research goal (description, prediction, causal inference), and identify the theoretical (typically non-measureable) quantity of interest.", "Identify the target measure for your analysis. Clearly specify the assumptions under which your research design and/or the data analysis allow your empirical estimand to answer the research question. Consider which variables to measure and to analyze (e.g., by a causal graph)", "Use the actual data to perform the pre-defined analysis. The final analysis may depend on features of the data or intermediate modeling results.", "Check the robustness of the modeling results. This should include checks on technical aspects (e.g., model convergence), statistical aspects (e.g., sensitivity), and content aspects (e.g., plausibility).", "Interpret the model results according to your research goal. Clearly communicate the assumptions made and the robustness of the results.")
)

flextable::qflextable(d2) |>
  flextable::set_header_labels(values = c("Stage", "Description")) |>
  flextable::width(width = c(2, 4)) #|>
  #ftExtra::colformat_md()

```

### Research Goal Formulation

All non-exploratory research projects should begin with a clear research goal. Research objectives typically fall into one of three categories: descriptive, predictive, or causal inference [@hern√°n2019]. While edge cases exist, categorizing the research aim into one of these categories helps guide subsequent steps in the research process, as each goal entails priorities and methods of analysis [@tredennick2021]. In sport science, consider an example from injury research to illustrate these distinctions [@nielsen2020]: Researchers might aim to *describe* the incidence of injuries in a specific athlete population, to *predict* future injuries based on training load and performance diagnostics, or estimate the *causal effect* an injury prevention program has on injury occurrence. Although these three questions could theoretically be addressed with the same (ideal) data set, the methods required to answer each question vary greatly based on the type of nature of the data analysis task.

Defining the explicit research goal before designing a study is crucial for implement goal-specific elements not only into the data analysis, but also into the research design. This does not mean that exploratory analyses are invalid or unreliable. Exploratory analyses are valuable for generating hypothesis and have historically led to many scientific discoveries. But exploratory analyses should be treated as what they are: These should be clearly communicated as exploratory rather than as confirmations of pre-existing hypothesis. Results from exploratory analyses should be interpreted with caution and ideally validated using other research methods[^discussion-7]. Exploratory analyses often arise from studies with non-exploratory aims, serving as supplementary findings. Nonetheless, establishing a clear research goal at the start of the research process remains essential. Any deviations from this goal should be clearly and transparently communicated.

[^discussion-7]: Unfortunately, merely stating the limitations of exploratory results when presenting them does not eliminate their inherent issues. The reader is left to navigate how to correctly interpret these results. For instance, many hypothesis-testing studies in sport science have low-statistical power [@mesquida2023]. It seems that underpowered research often gets published under the guise of a "pilot study", accompanied by a standard sentence in the limitations section noting the pilot nature of the trial and the need for further studies. As long as such studies are treated as evidence by the community, simply labeling analyses as "exploratory" may not substantially change their impact or interpretation.

The research question should translate into an target quantity of interest, known as the theoretical estimand [@lundberg2021; @kahan2024]. In causal research questions, the theoretical estimand often represents the outcome of a hypothetical intervention and is typically defined using potential outcomes[^discussion-8]. It is a hypothetical construct that cannot be directly measured. For example, if a researcher investigates the causal effect of a strength training intervention on running performance in a group of trained runners, the theoretical estimand is the difference in performance for an individual runner if they participated in the intervention versus if they did not. Since each runner can either participate or not, this measure remains unobservable. But it still serves as the theoretical target quantity of the research. Defining and stating the theoretical estimand helps formalize the research question and clarifies how and under which assumptions the subsequent analyses of actual measured quantities can answer the question.

[^discussion-8]: Following @lundberg2021, the theoretical estimand is formally defined as the unit-specific target quantity aggregated over a target population. Thus, the theoretical estimand consists of two components: the target quantity for each individual and the target group from which these quantities should be collected.

### Identifying an Empirical Estimand

The actual measurable target quantity in research is the empirical estimand [@lundberg2021]. The quantity is derived from the data collected given the methods used. Whether an empirical estimand actually allows to identify the theoretical estimand ‚Äì the ultimate, though unmeasurable, goal of the research ‚Äì depends on several assumptions.

To identify the necessary assumptions for causal research graphical causal models or other structures of causal knowledge are particularly useful. Following the previous example, in research examining the effect of strength training on running performance, the empirical estimand might be the mean group difference in running performance between an intervention and a control group. This differs from the theoretical estimand, which was the mean difference in performance for each individual. But under some assumptions, the mean group difference can be informative of the theoretical estimand we want to investigate. In this example, we might assume that the mean performance of the control group represents what the intervention group's performance would have been without the intervention, and vice versa. Randomization of treatment typically supports this assumption, but non-random treatment assignment can introduce bias into the empirical estimand. Issues such as measurement error and non-compliance (which might be addressed, for example, through an instrumental variable approach) or working with observational data, can further complicate matters. The main focus of this thesis, and causal inference research in general, is to provide methods for determining under which conditions empirical estimands can offer unbiased answers to causal questions.

### Estimation

Once an empirical estimand has been determined and justified, the actual data analysis process begins with the goal of estimating the desired quantity and its uncertainty based on the collected data. The estimation procedure itself is primarily a mathematical problem. Although the estimation should align with the identification method established earlier, it often does not require direct reference to the theoretical model underlying the research. For example, in traditional linear models, parameters can be estimated analytically. But other types of models might only offer numeric solution, meaning that different algorithms can yield different approximations (e.g., estimation procedures for non-linear models). This is particularly relevant for newer machine learning approaches or prediction problems, where model parameters are optimized to maximize goodness-of-fit rather than being derived from a theoretical model. Conceptually, it is useful to distinguish between this "estimation" model building and the previous "identification" model building, as both serve different roles in achieving the research goal.

### Supplementary Analyses as Robustness Checks

In many fields, including sport science, the results from a single chosen model are often accepted at the face value. This overlooks the reality that no single true model typically exists [@box1976]. Instead, a range of plausible models may be available, and researchers may select among them based on their outcomes [@ho2007]. Thus, researchers should not only present their one primary model for answering the research question but also provide empirical evidence supporting the credibility of this model through supplementary analyses [@athey2017].

Supplementary analyses can address any of the three previous stages of the modeling process (research question, empirical estimand, estimation). The robustness of research questions and theoretical estimand is rarely examined in a single research article, as it usually requires taking a totally different experimental approach. This is more commonly addressed through cumulative research across multiple projects. The robustness of empirical estimands, however, is what typical supplementary analyses focus on, which I will discuss in more detail in the following paragraph. Assessing the robustness of the estimation procedure, meanwhile, is a rather mathematical issue, focusing on whether the estimation process has been correctly executed. This is generally no issue for simple models (e.g., linear frequentist models), but becomes crucial for more complex models where checks for convergence and stability are necessary[^discussion-9].

[^discussion-9]: This is an important issue, for example, in machine learning, but also in Bayesian statistics. In Bayesian models fitted using Markov Chain Monte Carlo algorithms, appropriate diagnostics should be conducted to ensure that the resulting sample accurately depicts the target posterior distribution [@roy2020].

Traditional robustness analyses focus on the identification of the empirical estimand by examining the influence of plausible deviations in model specification. This involves assessing how model results change when different sets of variables are included [@patel2015], alternative preprocessing steps are applied [@steegen2016], or various functional forms of the model are used [@young2017]. A systematic approach is to develop a set of reasonable model specifications and analyze them collectively through a multimodel analysis [@steegen2016; @young2017]. Other methods include explicitly modeling unobserved confounders [e.g., @ding2016]. In predictive settings, cross-validation serves as a form of robustness checks. A completely different approach to assessing robustness in a supplementary analysis involves using data where the true parameters of interest are known. This can be done by either simulating data with known parameters [@jordon] and then running the model to verify correct parameter estimation. Alternatively, placebo analyses [@athey2017] can be conducted on data where from a theoretical view no meaningful effect is expected.

Performing supplementary analyses is an approach that highly depends on the exact research context. While individual assessments of research robustness have long been part of the scientific process, systematic methods such as multimodel analyses have been introduced only recently. Altough systematic supplementary analyses are still largely absent from sport science, they hold potential for enhancing the credibility of primary research findings.

### Interpreting and Communicating Results

The interpretation of research findings should align with the original research goals [@nielsen2020; @kezios2021]. This means that descriptive research should be interpreted as descriptive, predictive research as predictive, and causal research as causal. Accurate causal interpretation is particularly critical. Often, current research falls into the trap of indirect causal interpretation: researcher may recognize that their methodology does not support causal conclusion but use non-causal wording while still implicating causal meanings [@hern√°n2018]. For instance, studies might refer to "associations" to avoid the more causal terms "effects" or "relationship", even while implying causal effects in their discussions and summaries. This practice creates ambiguity[^discussion-10] [@haber2022], which can impede scientific progress [@grosz2020]. Sport science research is likely affected by similar issues of misaligned reporting and ambiguous causal language, though this needs to be confirmed by empirical investigations. Nonetheless, researchers should strive to clearly communicate their findings in sports science, with explicit attention to the scope and limitations of their research [@stovitz2019].

[^discussion-10]: The use of ambiguous causal language is rather not an attempt to mislead readers, but more the consequence of teaching and the current scientific system. Studies that are explicit about their causal intend (and its limitation) may have poorer chances to get through peer review, leading to even more studies being published with ambiguous causal language. This may lead to the observed paradox, that researchers view studies that are causally ambiguous as of higher quality and more practically relevant than studies with a clear causal language [@alvarez-vargas2023].
